{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "nhirwdf2bilsevyeby7o",
   "authorId": "7914099171838",
   "authorName": "CKOETAS",
   "authorEmail": "chris.koetas@ricoh-usa.com",
   "sessionId": "2871308e-96d7-4be3-a0f6-4ec2a27e07af",
   "lastEditTime": 1748547485786
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "Loading_Snowflake_Session"
   },
   "source": "from snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a245863a-21bf-419c-8342-b3a730e602e4",
   "metadata": {
    "language": "python",
    "name": "cell5",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "!pip install numpy==1.23.5 xgboost pandas==1.5.3 joblib==1.3.2 scikit-learn==1.3.2 tsfresh==0.20.1 category_encoders==2.6.2 scikit-optimize==0.9.0 statsmodels==0.13.5 scipy==1.10.1 tqdm==4.66.1 distributed==2023.3.1 dask==2023.3.1 shap",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9dab7256-ccc9-4e78-8bee-1332da564b21",
   "metadata": {
    "name": "cell20",
    "collapsed": false
   },
   "source": "# Import all required libraries"
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "Importing_Libraries"
   },
   "source": "import math\nimport pandas as pd\nimport numpy as np\nimport re\nimport os\nimport joblib\nimport sys\nimport sklearn\nimport skopt\nimport shap\nimport category_encoders\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom joblib import dump, load\nfrom datetime import datetime\nimport snowflake.snowpark as snowpark\nfrom snowflake.snowpark import Session\nfrom category_encoders.target_encoder import TargetEncoder\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nimport xgboost as xgb\n\nimport tsfresh\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom tsfresh.utilities.dataframe_functions import impute\nfrom tsfresh import extract_features, extract_relevant_features, select_features\nfrom tsfresh.utilities.dataframe_functions import impute\nfrom tsfresh.feature_extraction import ComprehensiveFCParameters\nfrom tsfresh.utilities.dataframe_functions import roll_time_series\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5550ab03-add9-472e-99f2-067ca44402f3",
   "metadata": {
    "language": "python",
    "name": "Library_Version",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "print(\"Python version\",sys.version)\nprint(\"Pandas version\", pd.__version__)\nprint(\"Numpy version\", np.__version__)\nprint(\"Scikit-Learn version\", sklearn.__version__)\nprint(\"Scikit-Optimize version\", skopt.__version__)\nprint(\"XGBoost version\", xgb.__version__)\nprint(\"Category Encoders version\", category_encoders.__version__)\nprint(\"Joblib version\", joblib.__version__)\nprint(\"Ts fresh version\", tsfresh.__version__)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1ea85f10-aa27-49ff-a57a-7619e95e4b64",
   "metadata": {
    "name": "cell47"
   },
   "source": "# UTILITY Function 1"
  },
  {
   "cell_type": "code",
   "id": "b740d828-ff1c-4d27-b09c-5d4bbc219013",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "def convert_to_float(df):\n    return df.apply(pd.to_numeric, errors='coerce').astype(np.float32)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "737f7bac-090f-4afe-a5e8-b8bcc5fe8437",
   "metadata": {
    "name": "cell48"
   },
   "source": "# UTILITY Function 2"
  },
  {
   "cell_type": "code",
   "id": "a09e5722-1238-4e65-9b6b-064add585da0",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": "def convert_yyyywk_to_date(yyyywk):\n    number_of_days_in_month = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n    time = str(yyyywk)\n    year = int(time[:4])\n    week_no = int(time[4:6])\n   \n    total_days = (week_no*7) - 3\n    days_count = 0\n    cummulative_sum = []\n    cummulative_sum.append(0)\n \n    for key,val in number_of_days_in_month.items():\n        days_count += val\n        if year%4 == 0 and key==2:\n            days_count+=1\n        cummulative_sum.append(days_count)\n \n    first = 0\n    second = 1\n    month = 12\n \n    while second < len(cummulative_sum):\n        if (cummulative_sum[first] <= total_days) and (total_days <= cummulative_sum[second]):\n            month = first+1\n            day = total_days-cummulative_sum[first]\n            break\n        else:\n            first+=1\n            second+=1\n \n    if month<10:\n        month_date = str(year)+\"-0\"+str(month)+\"-01\"\n    else:\n        month_date = str(year)+\"-\"+str(month)+\"-01\"\n    return month_date",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "94f00fef-4163-43b9-addc-dbc48a752e1a",
   "metadata": {
    "language": "python",
    "name": "cell78"
   },
   "outputs": [],
   "source": "def convert_yyyywk_to_actual_mid_date(yyyywk):\n    number_of_days_in_month = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n    time = str(yyyywk)\n    year = int(time[:4])\n    week_no = int(time[4:6])\n   \n    total_days = (week_no*7) - 3\n    days_count = 0\n    cummulative_sum = []\n    cummulative_sum.append(0)\n \n    for key,val in number_of_days_in_month.items():\n        days_count += val\n        if year%4 == 0 and key==2:\n            days_count+=1\n        cummulative_sum.append(days_count)\n \n    first = 0\n    second = 1\n    month = 12\n \n    while second < len(cummulative_sum):\n        if (cummulative_sum[first] <= total_days) and (total_days <= cummulative_sum[second]):\n            month = first+1\n            day = total_days-cummulative_sum[first]\n            break\n        else:\n            first+=1\n            second+=1\n    \n    day_no=\"00\"\n    \n    if day <10:\n        day_no = \"0\"+str(day)\n    else:\n        day_no = str(day)\n        \n    if month<10:\n        month_date = str(year)+\"-0\"+str(month)+\"-\"+day_no\n    else:\n        month_date = str(year)+\"-\"+str(month)+\"-\"+day_no\n    return month_date",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "55ae5fca-36e6-441e-a30f-7765484b434f",
   "metadata": {
    "language": "python",
    "name": "cell74"
   },
   "outputs": [],
   "source": "def get_days_diff(yyyywk, last_yyyywk):\n    curr = pd.to_datetime(convert_yyyywk_to_actual_mid_date(yyyywk))\n    last = pd.to_datetime(convert_yyyywk_to_actual_mid_date(last_yyyywk))\n    #print(curr, last)\n    days_diff = (last- curr) / np.timedelta64(1, 'D')\n    print(yyyywk, last_yyyywk, days_diff)\n    return days_diff",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "facde1c5-6f12-43a2-b4ef-6a5c2529741a",
   "metadata": {
    "language": "python",
    "name": "cell75"
   },
   "outputs": [],
   "source": "get_days_diff(202318, 202401)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "45b8f9ae-8dc3-42a3-991a-e22b8174a369",
   "metadata": {
    "name": "cell49"
   },
   "source": "# UTILITY Function 3"
  },
  {
   "cell_type": "code",
   "id": "3043d705-2564-4fa8-a38c-d225a70b2b72",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "def convert_date_to_yyyywk(d):\n        \n    number_of_days_in_month = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n    date = pd.to_datetime(d)\n    month = date.month\n    year = date.year\n    day = date.day\n    week_number = 1\n    days_count = 0\n    cummulative_sum = []\n    cummulative_sum.append(0)\n \n    for key,val in number_of_days_in_month.items():\n        days_count += val\n        if year%4 == 0 and key==2:\n            days_count+=1\n        cummulative_sum.append(days_count)\n    if month<13 and month>0:\n        week_number = math.ceil((cummulative_sum[month-1]+day)/7)\n        \n    yyyywk=\"000000\"\n    if week_number>9:\n        yyyywk = str(year)+str(week_number)\n    else:\n        yyyywk = str(year)+\"0\"+str(week_number)\n    ans = int(yyyywk)    \n    return ans",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "292febd4-ff50-4d6f-aaa1-95c8dc96daa3",
   "metadata": {
    "name": "cell93"
   },
   "source": "# UTILITY Function 4"
  },
  {
   "cell_type": "code",
   "id": "5415b857-32ba-441f-a991-51286349f4a8",
   "metadata": {
    "language": "python",
    "name": "cell92"
   },
   "outputs": [],
   "source": "def engineer_timeseries_cols_using_tsfresh_for_live_customers(only_features):\n    \n    final_features = pd.DataFrame()\n    \n    # Store Dataframe into CSV files for each customer id and dates\n\n    only_features['YYYYWK'] = only_features['YYYYWK'].astype(int)\n    only_features['CUST_ACCOUNT_NUMBER'] = only_features['CUST_ACCOUNT_NUMBER'].astype(int)\n    only_features = only_features.loc[:,~only_features.columns.str.contains('^Unnamed', case=False)]\n    only_features = only_features.fillna(0.0)\n\n    common_cust_id = only_features['CUST_ACCOUNT_NUMBER'].unique()\n    start_time_begin = time.time()\n    \n    count=1\n    for id_ in common_cust_id: \n        print(count)\n        start_time = time.time()\n        flag=False\n        \n        print(id_)    \n\n        only_ts_df = only_features[only_features['CUST_ACCOUNT_NUMBER'] == id_]\n        \n        average_df = only_ts_df.groupby('YYYYWK')[['INVOICE_REVLINE_TOTAL', 'ORIGINAL_AMOUNT_DUE', 'FUNCTIONAL_AMOUNT', 'USED_STORAGE_MB', 'DOCUMENTS_OPENED']].sum()\n        \n        average_df.reset_index(inplace=True)\n        average_df['CUST_ACCOUNT_NUMBER'] = id_\n\n        average_df['YYYYWK'] = average_df['YYYYWK'].astype(int)\n        average_df['CUST_ACCOUNT_NUMBER'] = average_df['CUST_ACCOUNT_NUMBER'].astype(int)\n    \n        #df_test = session.create_dataframe(average_df)\n        #df_test.write.copy_into_location(\"@PS_DOCUWARE_CHURN/only_ts_df\"+str(id_)+\".csv\", file_format_type=\"csv\", format_type_options={\"COMPRESSION\": \"NONE\"}, header=True, overwrite=True )\n        \n        only_ts_df = average_df\n        \n        if only_ts_df.shape[0] > 1:\n            df_rolled = roll_time_series(only_ts_df, column_id=\"CUST_ACCOUNT_NUMBER\", column_sort=\"YYYYWK\")          # roll_time_series\n        elif only_ts_df.shape[0] == 1:\n            flag=True\n            df_rolled = only_ts_df\n            df_rolled['id'] = \"(\"+str(only_ts_df.loc[only_ts_df.index[0],'CUST_ACCOUNT_NUMBER'])+\", \"+str(only_ts_df.loc[only_ts_df.index[0], 'YYYYWK'])+\")\"\n        else:\n            continue\n        \n        columns_to_dropped = set()\n\n        df_rolled_ = df_rolled.loc[:,~df_rolled.columns.str.contains('^(Unnamed|CUST_|YYYYWK|id)', case=False)]\n    \n        df_rolled_imputed = df_rolled_\n        \n        df_rolled_imputed['YYYYWK'] = df_rolled['YYYYWK'].to_list()\n        df_rolled_imputed['ID'] = df_rolled['id'].to_list()\n        df_rolled_imputed['CUST_ACCOUNT_NUMBER'] = df_rolled['CUST_ACCOUNT_NUMBER'].to_list()\n        #df_rolled_imputed.rename(columns={'USED_STORAGE__MB':'USED_STORAGE_MB'}, errors=\"raise\",inplace=True)\n        \n        columns_to_dropped = set()\n\n        df_rolled_imputed = df_rolled_imputed.loc[:,~df_rolled_imputed.columns.str.contains('^Unnamed', case=False)]\n    \n        extraction_settings = ComprehensiveFCParameters()\n\n        if df_rolled_imputed.shape[0] > 0:\n            features = extract_features(df_rolled_imputed, column_id='ID', column_sort='YYYYWK', default_fc_parameters=extraction_settings) #extract_features\n            \n            df_temp = (features.sort_index().iloc[features.shape[0]-1, :]).to_frame().transpose()\n            df_temp['CUST_ACCOUNT_NUMBER'] = id_\n        \n            final_features = pd.concat([final_features, df_temp], axis=0, ignore_index=True)\n            \n        del only_ts_df\n        del features\n        del df_rolled\n        del df_rolled_\n        del df_rolled_imputed\n        \n        end_time = time.time()\n        elapsed_time = end_time - start_time\n        #print(f\"for Cust ID = {id_} Elapsed time: {elapsed_time} seconds\")\n        count+=1\n        #if count == 11:\n        #    break\n    end_time_begin = time.time()\n    elapsed_time_begin = end_time_begin - start_time_begin\n    #print(f\"Total Elapsed time: {elapsed_time_begin} seconds\")\n\n    #Remove unwanted columns\n    final_features_columns = final_features.columns\n    columns_to_be_dropped_from_ts_df = []\n\n    for col in final_features_columns:\n        if re.search(r\"^(Unnamed:|obr_|prtar|CONTRACT_LINE_ITEMS|PROBABILITY_OF_DELINQUENCY|RICOH_CUSTOM_RISK_MODEL|CUST_ACCOUNT_NUMBER_)\", col):\n            columns_to_be_dropped_from_ts_df.append(col)\n\n    final_features_filtered = final_features.drop(columns_to_be_dropped_from_ts_df, axis=1)\n    \n    return final_features_filtered",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "454f0186-8cb6-4d3c-a5d3-5d296950fb34",
   "metadata": {
    "name": "cell50",
    "collapsed": false
   },
   "source": "# UTILITY Function 5"
  },
  {
   "cell_type": "code",
   "id": "ab8c7ee0-40c9-4561-88cd-3fe23a759f6d",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "def engineer_timeseries_cols_using_tsfresh(only_features):\n    \n    final_features = pd.DataFrame()\n    final_calculated = pd.DataFrame()\n    # Store Dataframe into CSV files for each customer id and dates\n\n    only_features['YYYYWK'] = only_features['YYYYWK'].astype(int)\n    only_features['CUST_ACCOUNT_NUMBER'] = only_features['CUST_ACCOUNT_NUMBER'].astype(int)\n    only_features = only_features.loc[:,~only_features.columns.str.contains('^Unnamed', case=False)]\n    only_features = only_features.fillna(0.0)\n\n    common_cust_id = only_features['CUST_ACCOUNT_NUMBER'].unique()\n    start_time_begin = time.time()\n    \n    count=1\n    for id_ in common_cust_id: \n        print(count)\n        start_time = time.time()\n        flag=False\n        \n        print(id_)    \n        calculated = pd.DataFrame(columns=['DAYS_REMAINING'])\n        only_ts_df = only_features[only_features['CUST_ACCOUNT_NUMBER'] == id_]\n        filename = \"@PS_DOCUWARE_CHURN/TSFRESH/all_feature_TSFRESH/only_ts_df_\"+str(id_)+\".csv\"\n        curr_df = session.create_dataframe(only_ts_df)\n        #curr_df.write.copy_into_location(\"@PS_DOCUWARE_CHURN/TSFRESH/all_feature_TSFRESH/only_ts_df\"+str(id_)+\".csv\", file_format_type=\"csv\", format_type_options={\"COMPRESSION\": \"NONE\"}, header=True, overwrite=True )\n        #curr_df.write.csv(filename, overwrite=True, single=True)\n        #session.file.put(only_ts_df, ,overwrite=True)\n        \n       \n        #print( calculated.shape )\n        sum_df = only_ts_df.groupby('YYYYWK')[['INVOICE_REVLINE_TOTAL', 'ORIGINAL_AMOUNT_DUE', 'FUNCTIONAL_AMOUNT', 'USED_STORAGE_MB', 'DOCUMENTS_OPENED']].sum()\n        \n        sum_df.reset_index(inplace=True)\n        sum_df['CUST_ACCOUNT_NUMBER'] = id_\n\n        sum_df['YYYYWK'] = sum_df['YYYYWK'].astype(int)\n        sum_df['CUST_ACCOUNT_NUMBER'] = sum_df['CUST_ACCOUNT_NUMBER'].astype(int)\n    \n        #df_test = session.create_dataframe(sum_df)\n        #df_test.write.copy_into_location(\"@PS_DOCUWARE_CHURN/only_ts_df\"+str(id_)+\".csv\", file_format_type=\"csv\", format_type_options={\"COMPRESSION\": \"NONE\"}, header=True, overwrite=True )\n        \n        only_ts_df = sum_df\n        df_test = session.create_dataframe(sum_df)\n        #df_test.write.copy_into_location(\"@PS_DOCUWARE_CHURN/TSFRESH/all_feature_TSFRESH/sum_ts_df\"+str(id_)+\".csv\", file_format_type=\"csv\", format_type_options={\"COMPRESSION\": \"NONE\"}, header=True, overwrite=True )\n        #print(\"1\", only_ts_df.shape)\n        \n        calculated['DAYS_REMAINING'] = only_ts_df['YYYYWK'].apply(get_days_diff, args=(only_ts_df.loc[only_ts_df.index[only_ts_df.shape[0]-1], 'YYYYWK'],))\n        # print(\"-------------------------------\")\n        # print(calculated['DAYS_REMAINING'].shape)  \n        # print(\"***********\")\n        # print(calculated['DAYS_REMAINING'])\n        # print(\"#####################\")\n        if only_ts_df.shape[0] > 1:\n            df_rolled = roll_time_series(only_ts_df, column_id=\"CUST_ACCOUNT_NUMBER\", column_sort=\"YYYYWK\")          # roll_time_series\n        elif only_ts_df.shape[0] == 1:\n            flag=True\n            df_rolled = only_ts_df\n            df_rolled['id'] = \"(\"+str(only_ts_df.loc[only_ts_df.index[0],'CUST_ACCOUNT_NUMBER'])+\", \"+str(only_ts_df.loc[only_ts_df.index[0], 'YYYYWK'])+\")\"\n        else:\n            continue\n        \n        columns_to_dropped = set()\n\n        df_rolled_ = df_rolled.loc[:,~df_rolled.columns.str.contains('^(Unnamed|CUST_|YYYYWK|id)', case=False)]\n    \n        df_rolled_imputed = df_rolled_\n        \n        df_rolled_imputed['YYYYWK'] = df_rolled['YYYYWK'].to_list()\n        df_rolled_imputed['ID'] = df_rolled['id'].to_list()\n        df_rolled_imputed['CUST_ACCOUNT_NUMBER'] = df_rolled['CUST_ACCOUNT_NUMBER'].to_list()\n        #df_rolled_imputed.rename(columns={'USED_STORAGE__MB':'USED_STORAGE_MB'}, errors=\"raise\",inplace=True)\n        \n        columns_to_dropped = set()\n\n        df_rolled_imputed = df_rolled_imputed.loc[:,~df_rolled_imputed.columns.str.contains('^Unnamed', case=False)]\n    \n        extraction_settings = ComprehensiveFCParameters()\n\n        if df_rolled_imputed.shape[0] > 0:\n            features = extract_features(df_rolled_imputed, column_id='ID', column_sort='YYYYWK', default_fc_parameters=extraction_settings) #extract_features\n            print(\"2\",features.shape)\n            #df_temp = (features.sort_index().iloc[features.shape[0]-1, :]).to_frame().transpose()\n            #df_temp = (features.sort_index().loc[:, :]).transpose()\n            #print(calculated['DAYS_REMAINING'])\n            features['CUST_ACCOUNT_NUMBER'] = id_\n            features = features.reset_index(drop=True)\n            print(\"3\",features.shape)\n            \n            #features_df = session.create_dataframe(features)\n            #features_df.write.copy_into_location(\"@PS_DOCUWARE_CHURN/TSFRESH/all_feature_TSFRESH/features_df\"+str(id_)+\".csv\", file_format_type=\"csv\", format_type_options={\"COMPRESSION\": \"NONE\"}, header=True, overwrite=True )\n        \n        \n            #features_with_age = pd.concat([features, calculated], axis=1, ignore_index=True)\n\n            #features_with_age_df = session.create_dataframe(features_with_age)\n            #features_with_age_df.write.copy_into_location(\"@PS_DOCUWARE_CHURN/TSFRESH/all_feature_TSFRESH/features_with_age_df\"+str(id_)+\".csv\", file_format_type=\"csv\", format_type_options={\"COMPRESSION\": \"NONE\"}, header=True, overwrite=True )\n\n            print(\"4\",features.shape)\n            final_features = pd.concat([final_features, features], axis=0, ignore_index=True)\n            final_calculated = pd.concat([final_calculated, calculated], axis=0, ignore_index=True)\n            #print(calculated['DAYS_REMAINING'])\n            \n            #final_features_df = session.create_dataframe(final_features)\n            #final_features_df.write.copy_into_location(\"@PS_DOCUWARE_CHURN/TSFRESH/all_feature_TSFRESH/final_features_df\"+str(id_)+\".csv\", file_format_type=\"csv\", format_type_options={\"COMPRESSION\": \"NONE\"}, header=True, overwrite=True )\n\n            print(\"5\", final_features.shape)\n            print(\"6\", final_calculated.shape)\n            \n        del only_ts_df\n        del features\n        del df_rolled\n        del df_rolled_\n        del df_rolled_imputed\n        del calculated\n        \n        end_time = time.time()\n        elapsed_time = end_time - start_time\n        #print(f\"for Cust ID = {id_} Elapsed time: {elapsed_time} seconds\")\n        count+=1\n        #if count==5:\n        #    break\n    end_time_begin = time.time()\n    elapsed_time_begin = end_time_begin - start_time_begin\n    #print(f\"Total Elapsed time: {elapsed_time_begin} seconds\")\n\n    #Remove unwanted columns\n    final_features_columns = final_features.columns\n    columns_to_be_dropped_from_ts_df = []\n    #print(final_features.columns)\n    for col in final_features_columns:\n        if re.search(r\"^(Unnamed:|obr_|prtar|CONTRACT_LINE_ITEMS|PROBABILITY_OF_DELINQUENCY|RICOH_CUSTOM_RISK_MODEL|CUST_ACCOUNT_NUMBER_)\", col):\n            columns_to_be_dropped_from_ts_df.append(col)\n\n    final_features_filtered = final_features.drop(columns_to_be_dropped_from_ts_df, axis=1)\n    \n    return final_features_filtered, final_calculated",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64e8d09b-7012-41b7-b775-c73ac2f2630b",
   "metadata": {
    "language": "python",
    "name": "cell76"
   },
   "outputs": [],
   "source": "#ts_comprehensive_df, ts_age_df = engineer_timeseries_cols_using_tsfresh(final_features_for_FE)\n#ts_comprehensive_df_sf = session.create_dataframe(ts_comprehensive_df)\n#ts_comprehensive_df_sf.write.copy_into_location(\"@PS_DOCUWARE_CHURN/TSFRESH/all_feature_TSFRESH/ts_comprehensive_df.csv\", file_format_type=\"csv\", format_type_options={\"COMPRESSION\": \"NONE\"}, header=True, overwrite=True )\n        \n#session.write_pandas(ts_comprehensive_df, \"PS_DOCUWARE_TSFRESH_FOR_TRAINING\", auto_create_table=True, overwrite = True)\n#session.write_pandas(ts_age_df, \"PS_DOCUWARE_TSFRESH_AGE\", auto_create_table=True, overwrite = True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6f2b9df5-8591-417e-b4c3-d113d48c7171",
   "metadata": {
    "language": "python",
    "name": "cell79"
   },
   "outputs": [],
   "source": "#ts_comprehensive_df.shape, ts_age_df.shape",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "38c559d1-bd8b-46fe-ad70-2b5487c30755",
   "metadata": {
    "language": "python",
    "name": "cell80"
   },
   "outputs": [],
   "source": "#ts_age_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ff1e5343-7df0-4701-adf9-37ce9e974d79",
   "metadata": {
    "language": "python",
    "name": "cell77"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b4f4ddec-9111-4934-a7f2-a5a8a89c070c",
   "metadata": {
    "name": "cell51",
    "collapsed": false
   },
   "source": "# UTILITY Function 6"
  },
  {
   "cell_type": "code",
   "id": "21bbc1fa-4cea-4612-b3cb-031c41962aa4",
   "metadata": {
    "language": "python",
    "name": "cell30"
   },
   "outputs": [],
   "source": "def global_shap_importance(model, X):\n    \"\"\" Return a dataframe containing the features sorted by Shap importance\n    Parameters\n    ----------\n    model : The tree-based model \n    X : pd.Dataframe\n         training set/test set/the whole dataset ... (without the label)\n    Returns\n    -------\n    pd.Dataframe\n        A dataframe containing the features sorted by Shap importance\n    \"\"\"\n    explainer = shap.Explainer(model)\n    shap_values = explainer(X)\n    cohorts = {\"\": shap_values}\n    cohort_labels = list(cohorts.keys())\n    cohort_exps = list(cohorts.values())\n    for i in range(len(cohort_exps)):\n        if len(cohort_exps[i].shape) == 2:\n            cohort_exps[i] = cohort_exps[i].abs.mean(0)\n    features = cohort_exps[0].data\n    feature_names = cohort_exps[0].feature_names\n    values = np.array([cohort_exps[i].values for i in range(len(cohort_exps))])\n    feature_importance = pd.DataFrame(\n        list(zip(feature_names, sum(values))), columns=['features', 'importance'])\n    feature_importance.sort_values(\n        by=['importance'], ascending=False, inplace=True)\n    return feature_importance",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a57288f4-920a-4d69-9038-1a0034e7e2e8",
   "metadata": {
    "name": "cell52"
   },
   "source": "# Raw Data Extraction"
  },
  {
   "cell_type": "code",
   "id": "601c82ba-5271-418e-95c5-c326887f9dfe",
   "metadata": {
    "language": "sql",
    "name": "Sproc_1"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE VIEW RUS_AIML.PS_DOCUWARE_PAYMENTS_V AS SELECT CUSTOMER_NO, RECEIPT_DATE, FUNCTIONAL_AMOUNT FROM RUS_AIML.PS_DOCUWARE_PAYMENTS;\n\nCREATE OR REPLACE VIEW RUS_AIML.PS_DOCUWARE_REVENUE_V AS SELECT CUST_ACCOUNT_NUMBER, DATE_INVOICE_GL_DATE, INVOICE_REVLINE_TOTAL FROM RUS_AIML.PS_DOCUWARE_REVENUE;\n\nCREATE OR REPLACE VIEW RUS_AIML.PS_DOCUWARE_TRX_V AS SELECT ACCOUNT_NUMBER, TRX_DATE, ORIGINAL_AMOUNT_DUE FROM RUS_AIML.PS_DOCUWARE_TRX;\n\nCREATE OR REPLACE VIEW RUS_AIML.PS_DOCUWARE_CONTRACTS_SUBLINE_V AS SELECT CUST_ACCOUNT_NUMBER, SLINE_START_DATE, SLINE_END_DATE, SLINE_STATUS FROM RUS_AIML.PS_DOCUWARE_CONTRACT_SUBLINE;\n\nCREATE OR REPLACE VIEW RUS_AIML.PS_DOCUWARE_RENEWALS_V AS SELECT TO_CHAR(BILLTOCUSTOMERNUMBER) AS BILLTOCUSTOMERNUMBER, TO_CHAR(SHIPTOCUSTNUM) AS SHIPTOCUSTNUM, STARTDATECOVERAGE, CONTRACT_END_DATE FROM RUS_AIML.PS_DOCUWARE_SSCD_RENEWALS;\n\nCREATE OR REPLACE VIEW RUS_AIML.PS_DOCUWARE_L1_CUST_V AS SELECT CUST_ACCOUNT_NUMBER, CUST_PARTY_NAME, L3_RISE_CONSOLIDATED_NUMBER, L3_RISE_CONSOLIDATED_NAME, L2_RISE_CONSOLIDATED_NUMBER, L2_RISE_CONSOLIDATED_NAME, CUST_ACCOUNT_TYPE, CUSTOMER_SEGMENT, CUSTOMER_SEGMENT_LEVEL, CHURNED_FLAG, CHURN_DATE FROM RUS_AIML.PS_DOCUWARE_L1_CUST;\n\nCREATE OR REPLACE VIEW RUS_AIML.DNB_RISK_BREAKDOWN_V AS SELECT TO_CHAR(ACCOUNT_NUMBER) AS ACCOUNT_NUMBER, OVERALL_BUSINESS_RISK, RICOH_CUSTOM_RISK_MODEL, PROBABILITY_OF_DELINQUENCY, PAYMENT_RISK_TRIPLE_A_RATING FROM RUS_AIML.DNB_RISK_BREAKDOWN;\n\nCREATE OR REPLACE VIEW RUS_AIML.DOCUWARE_USAGE_JAPAN_V1_LATEST_V AS\n WITH latback AS (\n   SELECT DISTINCT CONTRACT_NUMBER\n    ,REGEXP_REPLACE(trim(CUSTOMER_NAME), '  ', ' ') AS CUSTOMER_NAME\n    ,CONTRACT_END\n    ,DOCUMENTS_OPENED\n    ,USED_STORAGE__MB\n    ,CONTRACT_LINE_ITEMS\n    ,PERIOD\n    ,YYYYWK\n   FROM DOCUWARE_SHARED__REP_EUC1.PUBLIC.DOCUWARE_USAGE_JAPAN_V1_LATEST\n   WHERE RICOH_REGIONS = 'US/Canada'\n    AND ADP__CURRENT != 'Ricoh Canada Inc.'\n   \n   UNION ALL\n   \n   SELECT DISTINCT CONTRACT_NUMBER\n    ,REGEXP_REPLACE(trim(CUSTOMER_NAME), '  ', ' ') AS CUSTOMER_NAME\n    ,CONTRACT_END\n    ,DOCUMENTS_OPENED\n    ,USED_STORAGE__MB\n    ,CONTRACT_LINE_ITEMS\n    ,PERIOD\n    ,YYYYWK\n   FROM DOCUWARE_SHARED__REP_EUC1.PUBLIC.DOCUWARE_USAGE_JAPAN_V1_BACKUP\n   WHERE RICOH_REGIONS = 'US/Canada'\n    AND ADP__CURRENT != 'Ricoh Canada Inc.'\n   )\n  ,jaro AS (\n   SELECT DISTINCT a.CUST_ACCOUNT_NUMBER\n    ,b.CONTRACT_NUMBER\n    ,a.CUST_PARTY_NAME\n    ,b.CUSTOMER_NAME\n    ,b.CONTRACT_END\n    ,b.DOCUMENTS_OPENED\n    ,b.USED_STORAGE__MB\n    ,b.CONTRACT_LINE_ITEMS\n    ,b.PERIOD\n    ,b.YYYYWK\n    ,jarowinkler_similarity(a.CUST_PARTY_NAME, b.CUSTOMER_NAME)\n    ,rank() OVER (\n     PARTITION BY a.CUST_ACCOUNT_NUMBER ORDER BY jarowinkler_similarity(a.CUST_PARTY_NAME, b.CUSTOMER_NAME) DESC\n     ) AS match_rank\n   FROM RUS_AIML.PS_DOCUWARE_L1_CUST a\n   FULL OUTER JOIN latback b\n   WHERE jarowinkler_similarity(a.CUST_PARTY_NAME, b.CUSTOMER_NAME) BETWEEN 93\n     AND 100\n   )\nSELECT *\nFROM jaro\nWHERE jaro.match_rank = 1\n AND EXISTS (\n  SELECT 1\n  FROM jaro\n  );\n ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0016c620-4879-4a00-9d5a-b0648e864243",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "# Preprocessing Raw Data"
  },
  {
   "cell_type": "code",
   "id": "c92c205e-756d-4f21-8768-aae6519ca187",
   "metadata": {
    "language": "python",
    "name": "cell71"
   },
   "outputs": [],
   "source": "#merge_final",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a65221c4-09cc-4269-8116-3255f7d99aec",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": "#l1_cust_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "968c5c15-e939-48f1-af90-06fd29051e82",
   "metadata": {
    "language": "sql",
    "name": "cell14"
   },
   "outputs": [],
   "source": "-- SELECT * FROM RUS_AIML.PS_DOCUWARE_L1_CUST limit 2;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c80c3824-51e1-4224-aefc-e12b899d1c6c",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "# #Fetching from previous SPROC output table\n# raw_df = session.sql(\"SELECT * FROM RAC_RAPID_DEV.RUS_AIML.PS_DOCUWARE_RAW_DATA_EXTRACTION\").to_pandas()\n\n# non_ts_numeric_cols = [\"PROBABILITY_OF_DELINQUENCY\", \"RICOH_CUSTOM_RISK_MODEL\"]\n# non_ts_categorical_cols = [\"OVERALL_BUSINESS_RISK\", \"PAYMENT_RISK_TRIPLE_A_RATING\", \"CONTRACT_LINE_ITEMS\"]\n# columns_to_be_processed_later = non_ts_numeric_cols + non_ts_categorical_cols + [\"CUST_ACCOUNT_NUMBER\", \"LIFESPAN_MONTHS\"]\n# finalized_df_ohe_to_process = raw_df.groupby(\"CUST_ACCOUNT_NUMBER\")[columns_to_be_processed_later].first()\n\n# # Imputation for Non Time Series columns\n# pofd_median = finalized_df_ohe_to_process[\"PROBABILITY_OF_DELINQUENCY\"].median()\n# finalized_df_ohe_to_process[\"PROBABILITY_OF_DELINQUENCY\"] = finalized_df_ohe_to_process[\"PROBABILITY_OF_DELINQUENCY\"].apply(lambda x:   float(pofd_median) if np.isnan(x) else x)\n\n# temp=finalized_df_ohe_to_process[\"RICOH_CUSTOM_RISK_MODEL\"].mode().to_frame()\n# rcrm_mode = temp.loc[temp.index[0], 'RICOH_CUSTOM_RISK_MODEL']\n# finalized_df_ohe_to_process[\"RICOH_CUSTOM_RISK_MODEL\"] = finalized_df_ohe_to_process[\"RICOH_CUSTOM_RISK_MODEL\"].apply(lambda x: float(rcrm_mode) if np.isnan(x) else x)\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dc05557d-8e50-4e9f-80bb-39e7d78bbeae",
   "metadata": {
    "name": "cell53"
   },
   "source": "# Data Imputation and Feature Engineering to make Training Set"
  },
  {
   "cell_type": "code",
   "id": "254cf91d-3cb1-48e2-bf5a-0a3b6ab25568",
   "metadata": {
    "language": "python",
    "name": "Sproc_3"
   },
   "outputs": [],
   "source": "#try:\n#Fetching from previous SPROC output table\nraw_df = session.sql(\"SELECT * FROM RAC_RAPID_DEV.RUS_AIML.PS_DOCUWARE_RAW_DATA_EXTRACTION\").to_pandas()\n\nnon_ts_numeric_cols = [\"PROBABILITY_OF_DELINQUENCY\", \"RICOH_CUSTOM_RISK_MODEL\"]\nnon_ts_categorical_cols = [\"OVERALL_BUSINESS_RISK\", \"PAYMENT_RISK_TRIPLE_A_RATING\", \"CONTRACT_LINE_ITEMS\"]\ncolumns_to_be_processed_later = non_ts_numeric_cols + non_ts_categorical_cols + [\"CUST_ACCOUNT_NUMBER\", \"LIFESPAN_MONTHS\", \"DAYS_TO_CHURN\"]\nfinalized_df_ohe_to_process = raw_df.groupby(\"CUST_ACCOUNT_NUMBER\")[columns_to_be_processed_later].first()\n\n# Imputation for Non Time Series columns\npofd_median = finalized_df_ohe_to_process[\"PROBABILITY_OF_DELINQUENCY\"].median()\nfinalized_df_ohe_to_process[\"PROBABILITY_OF_DELINQUENCY\"] = finalized_df_ohe_to_process[\"PROBABILITY_OF_DELINQUENCY\"].apply(lambda x:   float(pofd_median) if np.isnan(x) else x)\n\n#rcrm_mode = finalized_df_ohe_to_process[\"RICOH_CUSTOM_RISK_MODEL\"].mode()\ntemp=finalized_df_ohe_to_process[\"RICOH_CUSTOM_RISK_MODEL\"].mode().to_frame()\nrcrm_mode = temp.loc[temp.index[0], 'RICOH_CUSTOM_RISK_MODEL']\nfinalized_df_ohe_to_process[\"RICOH_CUSTOM_RISK_MODEL\"] = finalized_df_ohe_to_process[\"RICOH_CUSTOM_RISK_MODEL\"].apply(lambda x: float(rcrm_mode) if np.isnan(x) else x)\n\n# One Hot Encoding for Categorical variables OVERALL_BUSINESS_RISK and PAYMENT_RISK_TRIPLE_A_RATING\nfinalized_df_ohe_to_process = finalized_df_ohe_to_process.reset_index(drop=True)\n\ncategorical_cols = [\"OVERALL_BUSINESS_RISK\", \"PAYMENT_RISK_TRIPLE_A_RATING\"]\nfill_value = \"UNK\"\n\nfor col in categorical_cols:\n    finalized_df_ohe_to_process[col].fillna(fill_value, inplace=True)\n    finalized_df_ohe_to_process[col] = finalized_df_ohe_to_process[col].str.replace(\" \", \"_\", regex=False)\n    \n    if col == \"OVERALL_BUSINESS_RISK\":\n        col_abreviation = \"obr_\"\n    else:\n        col_abreviation = \"prtar_\"\n        \n    le_ohe = LabelEncoder()\n    ohe = OneHotEncoder(handle_unknown = \"ignore\")\n    enc_train = le_ohe.fit_transform(finalized_df_ohe_to_process[col]).reshape(finalized_df_ohe_to_process.shape[0],1)\n    ohe_train = ohe.fit_transform(enc_train)\n    le_ohe_name_mapping = dict(zip(le_ohe.classes_, le_ohe.transform(le_ohe.classes_)))\n    \n    enc_train = finalized_df_ohe_to_process[col].map(le_ohe_name_mapping).ravel().reshape(-1,1)\n    enc_train[np.isnan(enc_train)] = 9999\n\n    cols = [col_abreviation + str(x) for x in le_ohe_name_mapping.keys()]\n    finalized_df_ohe_to_process = pd.concat([finalized_df_ohe_to_process.reset_index(), pd.DataFrame.sparse.from_spmatrix(ohe_train, columns = cols)], axis = 1).drop([\"index\"], axis=1)\n    finalized_df_ohe_to_process.drop([col], axis = 1, inplace=True)\n\ncolumns_to_be_droped = non_ts_categorical_cols+non_ts_numeric_cols\nraw_df.drop(columns_to_be_droped, axis=1, inplace=True)\n\n# Target Encoding for CONTRACT_LINE_ITEMS\nfinalized_df_ohe_to_process[\"CONTRACT_LINE_ITEMS\"].fillna(\"NA\", inplace=True)\nfinalized_df_ohe_to_process[\"CONTRACT_LINE_ITEMS\"] = finalized_df_ohe_to_process[\"CONTRACT_LINE_ITEMS\"].str.replace(r\"\\\\d+x \", \"\", regex=True)\n\nfor i, row in finalized_df_ohe_to_process.iterrows():\n    t = row[\"CONTRACT_LINE_ITEMS\"]\n    arr = t.split(\"-\")\n    arr = [x.strip() for x in arr]\n    arr_s = sorted(arr)\n    key = \"-\".join([s for s in arr_s])\n    finalized_df_ohe_to_process.loc[i, \"CONTRACT_LINE_ITEMS\"] = key\n\nX_train = finalized_df_ohe_to_process.copy()\ny_train = finalized_df_ohe_to_process[\"LIFESPAN_MONTHS\"]\n\nenc = TargetEncoder(cols = [\"CONTRACT_LINE_ITEMS\"]).fit(X_train, y_train)\nX_train_encoded = enc.transform(X_train)\n\nnow = datetime.now()\ndate_string = now.strftime(\"%m-%d-%Y\")\n\n#encoder_filename = \"@RAC_RAPID_DEV.RUS_AIML.PS_DOCUWARE_CHURN/ps_docuware_target_encoder_object/ENC_CURRENT.joblib\"\n#encoder_filename_ar = \"@RAC_RAPID_DEV.RUS_AIML.PS_DOCUWARE_CHURN/ps_docuware_target_encoder_object/archive/ENC_\"+date_string+\".joblib\"\n\n#joblib.dump(enc,  encoder_filename)\n#joblib.dump(enc, encoder_filename_ar)\n\nimport_dir = sys._xoptions.get(\"snowflake_import_directory\")\nmodel_file = os.path.join(\"/tmp\", \"ENC_CURRENT.joblib.gz\")\ndump(enc, model_file)\n#session.file.put(model_file, \"@PS_DOCUWARE_CHURN\",overwrite=True)\nsession.file.put(model_file, \"@PS_DOCUWARE_CHURN/ps_docuware_target_encoder_object\",overwrite=True)\n\nmodel_file_ar = os.path.join(\"/tmp\", \"ENC_\"+date_string+\".joblib.gz\")\ndump(enc, model_file_ar)\nsession.file.put(model_file_ar, \"@PS_DOCUWARE_CHURN/ps_docuware_target_encoder_object/archive\",overwrite=True)\n\n# Imputation for Time Series columns\nts_columns = [\"CUST_ACCOUNT_NUMBER\", \"YYYYWK\", \"DOCUMENTS_OPENED\", \"USED_STORAGE__MB\", \"INVOICE_REVLINE_TOTAL\", \"ORIGINAL_AMOUNT_DUE\", \"FUNCTIONAL_AMOUNT\"]\nraw_df[\"transformed_YYYYWK\"] = raw_df[\"MONTH\"].apply(convert_date_to_yyyywk)\n#Impute missing YYYYWK with equivalent MONTH\nraw_df[\"YYYYWK\"].fillna(raw_df[\"transformed_YYYYWK\"], inplace=True)\nraw_df.drop(\"transformed_YYYYWK\", axis=1, inplace=True)\nts_df = raw_df[ts_columns]\nts_df = ts_df[ts_df['YYYYWK'].notna()]\nts_df['YYYYWK'] = ts_df['YYYYWK'].astype(int)\nts_df['CUST_ACCOUNT_NUMBER'] = ts_df['CUST_ACCOUNT_NUMBER'].astype(int)\nts_df.rename(columns={'USED_STORAGE__MB':'USED_STORAGE_MB'},inplace=True)\nts_df_sorted = ts_df.sort_values(['CUST_ACCOUNT_NUMBER','YYYYWK']).drop_duplicates()\n\nonly_features = ts_df_sorted.copy()\nonly_features = only_features.fillna(0)\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8c186f38-530b-407b-ab77-3d8c9941434f",
   "metadata": {
    "language": "python",
    "name": "cell130"
   },
   "outputs": [],
   "source": "only_features.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d17f73c7-3b39-4123-8938-bac6fd6a6366",
   "metadata": {
    "language": "python",
    "name": "cell46"
   },
   "outputs": [],
   "source": "session.write_pandas(only_features, \"PS_DOCUWARE_RAW_TS_FEATURES\", auto_create_table=True, overwrite = True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7de320f4-3a2e-4560-8264-755b82f294ee",
   "metadata": {
    "language": "python",
    "name": "cell73"
   },
   "outputs": [],
   "source": "lifespan_df = raw_df[['CUST_ACCOUNT_NUMBER', 'DAYS_TO_CHURN']]\nlifespan_df['CUST_ACCOUNT_NUMBER'] = lifespan_df['CUST_ACCOUNT_NUMBER'].astype(int)\nlifespan_df.drop_duplicates(inplace=True)\nlifespan_df.shape",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "752056ee-9f5c-4a64-b83b-e566f1e7d61b",
   "metadata": {
    "language": "python",
    "name": "cell41"
   },
   "outputs": [],
   "source": "\"\"\"df_churn_cust = l1_cust_churned[[\"CUST_ACCOUNT_NUMBER\", \"CHURN_DATE\"]]\ndf_churn_cust['CHURN_YYYYWK_DATE'] = df_churn_cust['CHURN_DATE'].apply(convert_date_to_yyyywk)\ndf_churn_cust['CUST_ACCOUNT_NUMBER'] = df_churn_cust['CUST_ACCOUNT_NUMBER'].astype(int)\n#print(only_features.shape) # (14614, 7)\ndf_raw_churn = only_features.merge(df_churn_cust, on='CUST_ACCOUNT_NUMBER', how='inner')\n#print(df_raw_churn.shape) # (14614, 9)\nfinal_features = df_raw_churn[df_raw_churn['YYYYWK'] <= df_raw_churn['CHURN_YYYYWK_DATE']]\nprint(final_features.shape)\"\"\"  #commented out CK 5-19",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a6afc1fa-c0f5-499e-9ba0-ef1e3073264f",
   "metadata": {
    "language": "python",
    "name": "cell70"
   },
   "outputs": [],
   "source": "final_features.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "23485ae1-4248-4ba9-8b5e-3be571a260da",
   "metadata": {
    "language": "python",
    "name": "cell72"
   },
   "outputs": [],
   "source": "final_features_for_FE = final_features.drop(['CHURN_DATE', 'CHURN_YYYYWK_DATE'], axis=1)\n#print(final_features_for_FE.shape) # (5551, 7)\nfinal_features_for_FE = final_features_for_FE.merge(lifespan_df, on='CUST_ACCOUNT_NUMBER', how='inner')\n#print(final_features_for_FE.shape) # (5551, 8)\nfinal_features_for_FE.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e38fe153-4d58-435c-bf9c-8a859bf0e267",
   "metadata": {
    "language": "python",
    "name": "cell99"
   },
   "outputs": [],
   "source": "final_features_for_FE.head(5551)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce385d06-1b5d-4fca-b0dc-b278d9dbaad6",
   "metadata": {
    "language": "python",
    "name": "cell32"
   },
   "outputs": [],
   "source": "print(X_train_encoded['CONTRACT_LINE_ITEMS'])",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a49902a3-02ae-4d86-99ab-c77d74636c4e",
   "metadata": {
    "name": "cell55"
   },
   "source": "# Store time series and Non time series data into DB before feature engineering from TSFRESH"
  },
  {
   "cell_type": "code",
   "id": "ef6bd473-0d2c-4e88-9581-18d4986796ff",
   "metadata": {
    "language": "python",
    "name": "cell43"
   },
   "outputs": [],
   "source": "session.write_pandas(X_train_encoded, \"PS_DOCUWARE_RAW_X_TRAIN_ENCODED_DATA_BEFORE_TSFRESH\", auto_create_table=True, overwrite = True)\nsession.write_pandas(final_features_for_FE, \"PS_DOCUWARE_RAW_DATA_BEFORE_TSFRESH\", auto_create_table=True, overwrite = True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "791bf39c-10fd-4c91-8ee9-6d3f2d9c41f3",
   "metadata": {
    "language": "python",
    "name": "Sproc_2"
   },
   "outputs": [],
   "source": "#Fetching all the different views\npay_df = session.sql(\"SELECT * FROM RUS_AIML.PS_DOCUWARE_PAYMENTS_V\").to_pandas()\nrev_df = session.sql(\"SELECT * FROM RUS_AIML.PS_DOCUWARE_REVENUE_V\").to_pandas()\ntrx_df = session.sql(\"SELECT * FROM RUS_AIML.PS_DOCUWARE_TRX_V\").to_pandas()\ncontracts_sub_df = session.sql(\"SELECT * FROM RUS_AIML.PS_DOCUWARE_CONTRACTS_SUBLINE_V\").to_pandas()\nrenewals_df = session.sql(\"SELECT * FROM RUS_AIML.PS_DOCUWARE_RENEWALS_V\").to_pandas()\nl1_cust_df =  session.sql(\"SELECT * FROM RUS_AIML.PS_DOCUWARE_L1_CUST_V\").to_pandas()\ndnb_risk_df = session.sql(\"SELECT * FROM RUS_AIML.DNB_RISK_BREAKDOWN_V\").to_pandas()\nusage_latest = session.sql(\"SELECT * FROM RUS_AIML.DOCUWARE_USAGE_JAPAN_V1_LATEST_V\").to_pandas()\n\n# Removing duplicates from all the pandas dataframes\npay_df = pay_df.drop_duplicates()\nrev_df = rev_df.drop_duplicates()\ntrx_df = trx_df.drop_duplicates()\ncontracts_sub_df = contracts_sub_df.drop_duplicates()\nrenewals_df = renewals_df.drop_duplicates()\nl1_cust_df = l1_cust_df.drop_duplicates()\ndnb_risk_df = dnb_risk_df.drop_duplicates()\nusage_latest = usage_latest.drop_duplicates()\n\n# Selecting only churned customers\n\nl1_cust_churned = l1_cust_df[l1_cust_df[\"CHURNED_FLAG\"]==True]\n\npay_df_churned = pay_df.merge(l1_cust_churned[[\"CUST_ACCOUNT_NUMBER\"]], left_on = \"CUSTOMER_NO\", right_on = \"CUST_ACCOUNT_NUMBER\", how=\"inner\")\npay_df_churned = pay_df_churned.drop(\"CUSTOMER_NO\", axis=1)\npay_df_churned[\"MONTH\"] = pd.to_datetime(pay_df_churned[\"RECEIPT_DATE\"]).dt.to_period(\"M\").dt.to_timestamp()\n\nrev_df_churned = rev_df.merge(l1_cust_churned[[\"CUST_ACCOUNT_NUMBER\"]], on = \"CUST_ACCOUNT_NUMBER\", how=\"inner\")\nrev_df_churned[\"MONTH\"] = pd.to_datetime(rev_df_churned[\"DATE_INVOICE_GL_DATE\"]).dt.to_period(\"M\").dt.to_timestamp()\n\ntrx_df_churned = trx_df.merge(l1_cust_churned[[\"CUST_ACCOUNT_NUMBER\"]], left_on = \"ACCOUNT_NUMBER\", right_on = \"CUST_ACCOUNT_NUMBER\", how=\"inner\")\ntrx_df_churned = trx_df_churned.drop(\"ACCOUNT_NUMBER\", axis=1)\ntrx_df_churned[\"MONTH\"] = pd.to_datetime(trx_df_churned[\"TRX_DATE\"]).dt.to_period(\"M\").dt.to_timestamp()\n\np_r_t_merged = pay_df_churned.merge(rev_df_churned, on = [\"CUST_ACCOUNT_NUMBER\",\"MONTH\"], how=\"outer\").merge(trx_df_churned, on = [\"CUST_ACCOUNT_NUMBER\",\"MONTH\"], how=\"outer\")\n\n#Contracts Subline\ncontracts_sub_df[\"SLINE_START_DATE\"] = pd.to_datetime(contracts_sub_df[\"SLINE_START_DATE\"])\ncontracts_sub_df[\"SLINE_END_DATE\"] = pd.to_datetime(contracts_sub_df[\"SLINE_END_DATE\"])\ncontracts_sub_df[\"SUB_START_MONTH\"] = pd.to_datetime(contracts_sub_df[\"SLINE_START_DATE\"]).dt.to_period(\"M\").dt.to_timestamp()\ncontracts_sub_df[\"SUB_END_MONTH\"] = pd.to_datetime(contracts_sub_df[\"SLINE_END_DATE\"]).dt.to_period(\"M\").dt.to_timestamp()\ncontracts_sub_df[\"SUB_EARLIEST_MONTH\"] = contracts_sub_df.groupby(\"CUST_ACCOUNT_NUMBER\")[\"SUB_START_MONTH\"].transform(\"min\")\ncontracts_sub_df[\"SUB_LATEST_MONTH\"] = contracts_sub_df.groupby(\"CUST_ACCOUNT_NUMBER\")[\"SUB_END_MONTH\"].transform(\"max\")\ncontracts_sub_df_churned = contracts_sub_df.merge(l1_cust_churned[[\"CUST_ACCOUNT_NUMBER\"]], on = \"CUST_ACCOUNT_NUMBER\", how=\"inner\")\n\ncontracts_sub_df_churned = contracts_sub_df_churned.drop_duplicates()\n\n# Renewals\ncols_to_str = [\"BILLTOCUSTOMERNUMBER\",\"SHIPTOCUSTNUM\"]\nrenewals_df[cols_to_str] = renewals_df[cols_to_str].astype(\"Int64\").astype(str)\n\nrenewals_df_churned_1 = renewals_df.merge(l1_cust_churned[[\"CUST_ACCOUNT_NUMBER\"]], left_on = \"SHIPTOCUSTNUM\", right_on = \"CUST_ACCOUNT_NUMBER\",  how=\"left\")\n\nrenewals_df_churned_2 = renewals_df_churned_1.merge(l1_cust_churned[[\"CUST_ACCOUNT_NUMBER\"]], left_on = \"BILLTOCUSTOMERNUMBER\", right_on = \"CUST_ACCOUNT_NUMBER\",  how=\"left\", suffixes = (\"_1\", \"_2\"))\nrenewals_df_churned_2[\"CUST_ACCOUNT_NUMBER\"] = renewals_df_churned_2[\"CUST_ACCOUNT_NUMBER_1\"].fillna(renewals_df_churned_2[\"CUST_ACCOUNT_NUMBER_2\"])\nrenewals_df_churned_2 = renewals_df_churned_2.drop([\"BILLTOCUSTOMERNUMBER\", \"SHIPTOCUSTNUM\",\"CUST_ACCOUNT_NUMBER_1\",\"CUST_ACCOUNT_NUMBER_2\"], axis=1)\nrenewals_df_churned_2 = renewals_df_churned_2.dropna()\n\nrenewals_df_churned_2[\"STARTDATECOVERAGE\"] =  pd.to_datetime(renewals_df_churned_2[\"STARTDATECOVERAGE\"])\nrenewals_df_churned_2[\"RENEWALS_START_MONTH\"] = pd.to_datetime(renewals_df_churned_2[\"STARTDATECOVERAGE\"]).dt.to_period(\"M\").dt.to_timestamp()\nrenewals_df_churned_2[\"RENEWALS_END_MONTH\"] = pd.to_datetime(renewals_df_churned_2[\"CONTRACT_END_DATE\"]).dt.to_period(\"M\").dt.to_timestamp()\n\nrenewals_df_churned_2[\"RENEWALS_EARLIEST_MONTH\"] = renewals_df_churned_2.groupby(renewals_df_churned_2[\"CUST_ACCOUNT_NUMBER\"])[\"RENEWALS_START_MONTH\"].transform(\"min\")\nrenewals_df_churned_2[\"RENEWALS_LATEST_MONTH\"] = renewals_df_churned_2.groupby(renewals_df_churned_2[\"CUST_ACCOUNT_NUMBER\"])[\"RENEWALS_END_MONTH\"].transform(\"max\")\n\n#DNB Risk Breakdown\ndnb_risk_df[\"ACCOUNT_NUMBER\"] = dnb_risk_df[\"ACCOUNT_NUMBER\"].astype(\"Int64\").astype(str)\ndnb_risk_df_churned = dnb_risk_df.merge(l1_cust_churned[[\"CUST_ACCOUNT_NUMBER\"]], left_on = \"ACCOUNT_NUMBER\", right_on = \"CUST_ACCOUNT_NUMBER\",  how=\"inner\")\ndnb_risk_df_churned = dnb_risk_df_churned.drop(\"ACCOUNT_NUMBER\", axis=1)\n\nusage_latest[\"YYYYWK_Transformed\"] = pd.to_datetime(usage_latest[\"YYYYWK\"].apply(convert_yyyywk_to_date), errors = \"coerce\")\n\n#Usage Japan Latest\nusage_latest[\"CUST_ACCOUNT_NUMBER\"] = usage_latest[\"CUST_ACCOUNT_NUMBER\"].astype(\"Int64\").astype(str)\nusage_latest[\"YYYYWK_MONTH\"] = pd.to_datetime(usage_latest[\"YYYYWK_Transformed\"]).dt.to_period(\"M\").dt.to_timestamp()\nusage_latest_churned = usage_latest.merge(l1_cust_churned[[\"CUST_ACCOUNT_NUMBER\"]], on=\"CUST_ACCOUNT_NUMBER\",  how=\"inner\")\n\n# Merging all the churned customers data frames i.e. Payments, Revenue, Transactions, contracts, contracts subline, contracts topline, renewals, snow inc, tech survey, loyalty survey, dnb risk and usage latest\n\nmerged_1 = p_r_t_merged.merge(contracts_sub_df_churned, left_on = [\"CUST_ACCOUNT_NUMBER\", \"MONTH\"], right_on = [\"CUST_ACCOUNT_NUMBER\", \"SUB_START_MONTH\"], how=\"outer\")\nmerged_1[\"MONTH\"] = merged_1[\"MONTH\"].fillna(merged_1[\"SUB_START_MONTH\"])\nmerged_1 = merged_1.drop(\"SUB_START_MONTH\", axis=1)\n\nmerged_2 = merged_1.merge(renewals_df_churned_2, left_on = [\"CUST_ACCOUNT_NUMBER\",\"MONTH\"], right_on = [\"CUST_ACCOUNT_NUMBER\",\"RENEWALS_START_MONTH\"], how=\"outer\")\nmerged_2[\"MONTH\"] = merged_2[\"MONTH\"].fillna(merged_2[\"RENEWALS_START_MONTH\"])\nmerged_2 = merged_2.drop(\"RENEWALS_START_MONTH\", axis=1)\n\nmerged_3 = merged_2.merge(dnb_risk_df_churned, on=\"CUST_ACCOUNT_NUMBER\", how=\"left\")\n\nmerged_4 = merged_3.merge(usage_latest_churned, left_on= [\"CUST_ACCOUNT_NUMBER\",\"MONTH\"], right_on = [\"CUST_ACCOUNT_NUMBER\", \"YYYYWK_MONTH\"], how=\"outer\")\nmerged_4[\"MONTH\"] = merged_4[\"MONTH\"].fillna(merged_4[\"YYYYWK_MONTH\"])\n\nto_drop_2 = [\"CONTRACT_NUMBER\", \"CUST_PARTY_NAME\", \"CUSTOMER_NAME\", \"CONTRACT_END\",\"JAROWINKLER_SIMILARITY(A.CUST_PARTY_NAME, B.CUSTOMER_NAME)\"]\n\nmerged_4 = merged_4.drop(to_drop_2, axis=1)\n\nmerged_5 = merged_4.merge(l1_cust_churned[[\"CUST_ACCOUNT_NUMBER\", \"CHURNED_FLAG\", \"CHURN_DATE\"]], on = \"CUST_ACCOUNT_NUMBER\", how=\"inner\")\n\nmerged_5[\"EARLIEST_DATE\"] = merged_5[[\"RECEIPT_DATE\", \"DATE_INVOICE_GL_DATE\", \"TRX_DATE\", \"SLINE_START_DATE\", \"STARTDATECOVERAGE\"]].min(axis=1)\nmerged_5[\"FINAL_EARLIEST_DATE\"] = merged_5.groupby(\"CUST_ACCOUNT_NUMBER\")[\"EARLIEST_DATE\"].transform(\"min\")\n\nmerged_5[\"CHURN_DATE\"] = pd.to_datetime(merged_5[\"CHURN_DATE\"])\nmerged_5[\"CHURN_MONTH\"] = pd.to_datetime(merged_5[\"CHURN_DATE\"]).dt.to_period(\"M\").dt.to_timestamp()\n\n\nmerged_5[\"LIFESPAN_MONTHS\"] = ((merged_5[\"CHURN_DATE\"] - merged_5[\"FINAL_EARLIEST_DATE\"]).dt.days) / 30\nmerged_5[\"DAYS_TO_CHURN\"]  = ((merged_5[\"CHURN_DATE\"] - merged_5[\"FINAL_EARLIEST_DATE\"]).dt.days)\n\nto_drop_3 = [\"RECEIPT_DATE\", \"DATE_INVOICE_GL_DATE\", \"TRX_DATE\", \"SLINE_START_DATE\", \"SLINE_END_DATE\", \"SUB_END_MONTH\", \"SLINE_STATUS\", \"SUB_EARLIEST_MONTH\", \"SUB_LATEST_MONTH\", \"STARTDATECOVERAGE\", \"CONTRACT_END_DATE\", \"RENEWALS_END_MONTH\", \"RENEWALS_EARLIEST_MONTH\",\"RENEWALS_LATEST_MONTH\", \"YYYYWK_MONTH\", \"PERIOD\", \"CHURNED_FLAG\",\"CHURN_MONTH\", \"EARLIEST_DATE\"]\n\nmerged_5 = merged_5.drop(to_drop_3, axis=1)\n\n\ndate_col = [\"MONTH\",\"YYYYWK_Transformed\", \"FINAL_EARLIEST_DATE\", \"CHURN_DATE\"]\n\nmerged_5[date_col] = merged_5[date_col].astype(str)\nmerged_5 = merged_5.drop_duplicates()\n\nusage_latest_churned['CUST_ACCOUNT_NUMBER'].unique()\n\ntemp = pd.DataFrame({'CUST_ACCOUNT_NUMBER':usage_latest_churned['CUST_ACCOUNT_NUMBER'].unique()})\nmerge_final = merged_5.merge(temp, on='CUST_ACCOUNT_NUMBER', how='inner')\n\nsession.write_pandas(merge_final, \"PS_DOCUWARE_RAW_DATA_EXTRACTION\", auto_create_table=True, overwrite = True)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d1316a04-aa64-454d-afdf-6689b2dd1bb6",
   "metadata": {
    "name": "cell56"
   },
   "source": "# Starting TSFRESH API"
  },
  {
   "cell_type": "code",
   "id": "5dc0ae5f-ecee-4554-821e-a79e24cc6dca",
   "metadata": {
    "language": "python",
    "name": "cell42"
   },
   "outputs": [],
   "source": "#ts_comprehensive_df = time_series_ts_fresh_features(only_features)\nts_comprehensive_df, final_ts_age  = engineer_timeseries_cols_using_tsfresh(final_features_for_FE)\n\n#ts_comprehensive_df = session.sql(\"SELECT * FROM RAC_RAPID_DEV.RUS_AIML.PS_DOCUWARE_ONLY_TS_DF\").to_pandas()\n\n    \n#except Exception as e: \n#    print(\"FAIL(Post Processing)!\" + \" Error: \" + str(e))\nts_comprehensive_df = ts_comprehensive_df.reset_index(drop=True)\nfinal_ts_age = final_ts_age.reset_index(drop=True)\n\nfinal_ts_df = pd.concat([ts_comprehensive_df, final_ts_age], axis=1, ignore_index=True)\n#session.write_pandas(final_ts_df, \"PS_DOCUWARE_TSFRESH_FEATURES_WITH_AGE\", auto_create_table=True, overwrite = True)\n\nall_columns = ts_comprehensive_df.columns.tolist()+final_ts_age.columns.tolist()\nfinal_ts_df.columns = all_columns\nfinal_ts_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e62ec712-7855-497f-a445-fc9f0c697e21",
   "metadata": {
    "language": "python",
    "name": "cell83"
   },
   "outputs": [],
   "source": "final_ts_age.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a07721c3-e9b1-4b8a-8085-3c7dcc60be9f",
   "metadata": {
    "language": "python",
    "name": "cell84"
   },
   "outputs": [],
   "source": "for col in final_ts_df.columns:\n    if re.search(\"^DAYS\", col):\n        print(col)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "056d029c-dcb0-4365-b7ff-6fed799328f5",
   "metadata": {
    "language": "python",
    "name": "cell38"
   },
   "outputs": [],
   "source": "ts_comprehensive_df = final_ts_df.copy()\nts_comprehensive_df.shape",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "173ce17f-bd3c-4b07-aa54-bb3b8f78c7eb",
   "metadata": {
    "language": "python",
    "name": "cell39"
   },
   "outputs": [],
   "source": "# Check if it contains CUST_ACCOUNT_NUMBER related TSFRESH features\n\nfor col in ts_comprehensive_df.columns:\n    if re.search(\"^CUST_ACCOUNT_NUMBER_\", col):\n        print(col)\n    if re.search(\"^DAYS\", col):\n        print(col)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c6e727bb-c2b7-4d21-b9a6-82ff001de78c",
   "metadata": {
    "language": "python",
    "name": "cell37",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Check first two words of all columns generated by TSFRESH\nprefix=set()\n\nfor col in ts_comprehensive_df.columns:\n    prefix.add((col.split('_')[0], col.split('_')[1]))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ddf530b0-dc2d-4e64-bf30-0b524a00a2bc",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": "print(prefix)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "07e9a0e5-dad2-402a-9b8d-65e34e193d05",
   "metadata": {
    "name": "cell54",
    "collapsed": false
   },
   "source": "# Storing Training Data after Feature Engineering and before Training"
  },
  {
   "cell_type": "code",
   "id": "a37b251e-8ba0-46a3-8aaf-492e9b580720",
   "metadata": {
    "language": "python",
    "name": "cell36"
   },
   "outputs": [],
   "source": "ts_comprehensive_df[\"CUST_ACCOUNT_NUMBER\"] = ts_comprehensive_df[\"CUST_ACCOUNT_NUMBER\"].astype(int)\nX_train_encoded[\"CUST_ACCOUNT_NUMBER\"] = X_train_encoded[\"CUST_ACCOUNT_NUMBER\"].astype(int)\nX_train_encoded = X_train_encoded.reset_index(drop=True)\nts_comprehensive_df = ts_comprehensive_df.reset_index(drop=True)\ncomprehensive_imputed_df = pd.merge(ts_comprehensive_df, X_train_encoded, on=\"CUST_ACCOUNT_NUMBER\", how=\"inner\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9cb47cef-abf4-4bbc-8c3a-8664fb682fb6",
   "metadata": {
    "language": "python",
    "name": "cell91"
   },
   "outputs": [],
   "source": "ts_comprehensive_df.shape, X_train_encoded.shape, comprehensive_imputed_df.shape",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4339a5b5-58dd-4ae1-b473-5fe3f1cd8079",
   "metadata": {
    "language": "python",
    "name": "cell89"
   },
   "outputs": [],
   "source": "for col in ts_comprehensive_df.columns:\n    if re.search(\"^DAYS\", col):\n        print(col)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "11d39518-4346-4625-90e1-e78e31e508c7",
   "metadata": {
    "language": "python",
    "name": "cell90"
   },
   "outputs": [],
   "source": "for col in comprehensive_imputed_df.columns:\n    if re.search(\"^DAYS\", col):\n        print(col)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a0f78b5-f8d3-4dfd-8712-82d369534624",
   "metadata": {
    "language": "python",
    "name": "cell88"
   },
   "outputs": [],
   "source": "for col in comprehensive_imputed_df.columns:\n    if pd.api.types.is_sparse(comprehensive_imputed_df[col]):\n        comprehensive_imputed_df[col] = comprehensive_imputed_df[col].sparse.to_dense()\n\nsession.write_pandas(comprehensive_imputed_df, \"PS_DOCUWARE_IMPUTED_DATA\", auto_create_table=True, overwrite = True)\n#session.write_pandas(comprehensive_imputed_df, \"PS_DOCUWARE_IMPUTED_DATA_266\", auto_create_table=True, overwrite = True)\n\nprint(\"Successfuly created table PS_DOCUWARE_IMPUTED_DATA\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c6fed6f2-c4fc-4f25-861d-20b11b20b475",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": "ts_comprehensive_df.shape\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aea1afa8-ca44-4958-8fe4-29ef5135696a",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "comprehensive_imputed_df.shape",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0292dfe0-0ede-4948-b694-6d9058112deb",
   "metadata": {
    "language": "python",
    "name": "cell86"
   },
   "outputs": [],
   "source": "for col in ts_comprehensive_df.columns:\n    if re.search(\"^DAYS\", col):\n        print(col)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "92071196-ea2c-43ef-b02f-252dbe77c914",
   "metadata": {
    "language": "python",
    "name": "cell87"
   },
   "outputs": [],
   "source": "for col in X_train_encoded.columns:\n    if re.search(\"^DAYS\", col):\n        print(col)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ead33bef-c88a-4aa4-8e60-81b21a6b71d7",
   "metadata": {
    "language": "python",
    "name": "cell85"
   },
   "outputs": [],
   "source": "for col in comprehensive_imputed_df.columns:\n    if re.search(\"^DAYS\", col):\n        print(col)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e03e5d5d-05fe-4144-8772-499ed108e043",
   "metadata": {
    "language": "python",
    "name": "cell81"
   },
   "outputs": [],
   "source": "# Check first two words of all columns generated by TSFRESH\nprefix=set()\n\nfor col in comprehensive_imputed_df.columns:\n    prefix.add((col.split('_')[0], col.split('_')[1]))\n    prefix.add(col.split('_')[0])",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1c2ddb7e-9f90-43f7-af8c-251ab00b2399",
   "metadata": {
    "language": "python",
    "name": "cell82"
   },
   "outputs": [],
   "source": "prefix",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "091688e3-6f25-40c6-9b24-c8f70cfff760",
   "metadata": {
    "name": "cell57"
   },
   "source": "# Training using XGBoost Regression"
  },
  {
   "cell_type": "code",
   "id": "f9788ab3-3537-47bb-884b-53846d7406e0",
   "metadata": {
    "language": "python",
    "name": "cell100"
   },
   "outputs": [],
   "source": "comprehensive_imputed_df = session.sql(\"SELECT * FROM RUS_AIML.PS_DOCUWARE_IMPUTED_DATA\").to_pandas()\ncomprehensive_imputed_df = comprehensive_imputed_df.replace([np.inf, -np.inf], 0)\ncomprehensive_imputed_df = comprehensive_imputed_df.fillna(0)\n\nfor col in comprehensive_imputed_df.columns:\n    comprehensive_imputed_df[col] = pd.to_numeric(comprehensive_imputed_df[col], errors=\"coerce\").astype(float)\n\n# comprehensive_imputed_o_df = pd.series(comprehensive_imputed_df)\n# comprehensive_imputed_df = pd.to_numeric(comprehensive_imputed_o_df, errors=\"coerce\")\n# comprehensive_imputed_df = comprehensive_imputed_df.to_frame()\n\n\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e529ed9-e142-43b8-b9f8-f380850f4a4a",
   "metadata": {
    "language": "python",
    "name": "cell101"
   },
   "outputs": [],
   "source": "cust_churn_df = comprehensive_imputed_df[[\"CUST_ACCOUNT_NUMBER\",\"DAYS_TO_CHURN\"]]\ncust_churn_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec37c080-e714-443c-bf26-4a67461d6af6",
   "metadata": {
    "language": "python",
    "name": "cell103"
   },
   "outputs": [],
   "source": "cust_churn_df.drop_duplicates(inplace=True)\ncust_churn_df.shape",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5e4ee3f-737b-4ae3-ba46-0c37f2a251a6",
   "metadata": {
    "language": "python",
    "name": "cell104"
   },
   "outputs": [],
   "source": "cust_churn_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "385f87fb-11db-4241-80d5-69c5bb94f697",
   "metadata": {
    "language": "python",
    "name": "cell102"
   },
   "outputs": [],
   "source": "# Stratified Sampling\nstratified_df = pd.DataFrame()\n\nlabel_count=1\nfor i in range(365, 1461, 365):\n    XX = cust_churn_df[((i-365) < cust_churn_df[\"DAYS_TO_CHURN\"]) & (cust_churn_df[\"DAYS_TO_CHURN\"] <= i)]\n    XX[\"SAMPLE\"] = label_count\n    stratified_df = pd.concat([stratified_df, XX], axis=0, ignore_index=True)\n    label_count+=1\n    #print(\"For \", (i-365)+1, \" To \", i , \" Total Counts = \", XX.shape[0])\n    if i==1460:\n        XX = cust_churn_df[1460 < cust_churn_df[\"DAYS_TO_CHURN\"]]\n        XX[\"SAMPLE\"] = label_count\n        stratified_df = pd.concat([stratified_df, XX], axis=0, ignore_index=True)\n        #print(\"For \", 1460+1, \" To upper limit Total Counts = \", (1460 < data[\"DAYS_TO_CHURN\"]).sum())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "72535609-7573-4fac-a161-cec99b4c2a87",
   "metadata": {
    "language": "python",
    "name": "cell128"
   },
   "outputs": [],
   "source": "stratified_df[\"NO_OF_RENEWALS\"] = stratified_df[\"SAMPLE\"]-1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f3f7dd8c-b2bf-495c-81ba-dd66eb8986b7",
   "metadata": {
    "language": "python",
    "name": "cell105"
   },
   "outputs": [],
   "source": "stratified_df[\"SAMPLE\"].value_counts()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34255f3e-e685-454d-b4de-37a0233cc25f",
   "metadata": {
    "language": "python",
    "name": "cell129"
   },
   "outputs": [],
   "source": "stratified_df[\"NO_OF_RENEWALS\"].value_counts()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34b0eee2-5099-4605-b044-d586d291e4a8",
   "metadata": {
    "language": "python",
    "name": "cell108"
   },
   "outputs": [],
   "source": "stratified_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5d12ff90-9d23-4225-9603-3b7d1a5c44c3",
   "metadata": {
    "language": "python",
    "name": "cell106"
   },
   "outputs": [],
   "source": "\n\nfeatures = stratified_df.drop([\"DAYS_TO_CHURN\", \"SAMPLE\"], axis=1)\ny = stratified_df[\"SAMPLE\"]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "df96fa26-ecff-4839-92fa-e036d8ed7b3e",
   "metadata": {
    "language": "python",
    "name": "cell107"
   },
   "outputs": [],
   "source": "X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.2, random_state=42, shuffle=True, stratify=y)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46a01239-9a33-4d7f-97d6-9a4fa24ee7fb",
   "metadata": {
    "language": "python",
    "name": "cell109"
   },
   "outputs": [],
   "source": "X_train.shape, X_test.shape, y_train.shape, y_test.shape",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e3d54964-85b0-4584-bfaa-ce67b8afda4b",
   "metadata": {
    "language": "python",
    "name": "cell110"
   },
   "outputs": [],
   "source": "y_train.value_counts()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18077aa3-2b3e-4495-a37e-1aef1c4f7b3d",
   "metadata": {
    "language": "python",
    "name": "cell111"
   },
   "outputs": [],
   "source": "y_test.value_counts()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7fa09f68-35d5-41f0-b55f-47d2971b3c17",
   "metadata": {
    "language": "python",
    "name": "cell112"
   },
   "outputs": [],
   "source": "X_test.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6e885d7a-8de0-4603-8485-9c2c696cfcee",
   "metadata": {
    "language": "python",
    "name": "cell113"
   },
   "outputs": [],
   "source": "type(X_train), type(X_test), type(y_train), type(y_test)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "01e9923b-a6ee-45f6-b43b-5bab2b177529",
   "metadata": {
    "language": "python",
    "name": "cell114"
   },
   "outputs": [],
   "source": "comprehensive_imputed_df.shape",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a9d0b202-e623-4855-8eba-33eae0ae2fcf",
   "metadata": {
    "language": "python",
    "name": "cell115"
   },
   "outputs": [],
   "source": "X_train_all_cols = pd.merge(comprehensive_imputed_df, X_train, how='inner', on='CUST_ACCOUNT_NUMBER')\nX_train_all_cols.shape",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "691786db-ffd1-4361-8bb1-c2c443a4bca0",
   "metadata": {
    "language": "python",
    "name": "cell116"
   },
   "outputs": [],
   "source": "X_test_all_cols = pd.merge(comprehensive_imputed_df, X_test, how='inner', on='CUST_ACCOUNT_NUMBER')\nX_test_all_cols.shape",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "add708fe-f990-44cd-9615-f122a0793496",
   "metadata": {
    "language": "python",
    "name": "cell117"
   },
   "outputs": [],
   "source": "train_X = X_train_all_cols.drop([\"CUST_ACCOUNT_NUMBER\",\"LIFESPAN_MONTHS\",\"DAYS_TO_CHURN\", \"DAYS_REMAINING\"], axis=1) # Let cluster be in training\ntrain_y = X_train_all_cols[\"DAYS_REMAINING\"]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a6866401-320f-49c8-9085-2c1f3c444497",
   "metadata": {
    "language": "python",
    "name": "cell118"
   },
   "outputs": [],
   "source": "test_X = X_test_all_cols.drop([\"CUST_ACCOUNT_NUMBER\",\"LIFESPAN_MONTHS\",\"DAYS_TO_CHURN\", \"DAYS_REMAINING\"], axis=1) # Let cluster be in training\ntest_y = X_test_all_cols[\"DAYS_REMAINING\"]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d0a5af01-1b4f-418e-af31-564f95f8e8a7",
   "metadata": {
    "language": "python",
    "name": "cell119"
   },
   "outputs": [],
   "source": "train_X.shape, train_y.shape, test_X.shape, test_y.shape",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f152e552-679c-4e21-a496-6826b5f85091",
   "metadata": {
    "language": "python",
    "name": "cell120"
   },
   "outputs": [],
   "source": "type(train_X), type(test_X), type(train_y), type(test_y)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e974b800-3696-4cab-870c-f9a2cc6d144d",
   "metadata": {
    "language": "python",
    "name": "Sproc_4",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# #try:\n# #Fetching from previous SPROC output table\n# comprehensive_imputed_df = session.sql(\"SELECT * FROM RUS_AIML.PS_DOCUWARE_IMPUTED_DATA\").to_pandas()\n# comprehensive_imputed_df = comprehensive_imputed_df.replace([np.inf, -np.inf], 0)\n# comprehensive_imputed_df = comprehensive_imputed_df.fillna(0)\n\n# for col in comprehensive_imputed_df.columns:\n#     comprehensive_imputed_df[col] = pd.to_numeric(comprehensive_imputed_df[col], errors=\"coerce\").astype(float)\n\n# # comprehensive_imputed_o_df = pd.series(comprehensive_imputed_df)\n# # comprehensive_imputed_df = pd.to_numeric(comprehensive_imputed_o_df, errors=\"coerce\")\n# # comprehensive_imputed_df = comprehensive_imputed_df.to_frame()\n\n# # Stratified Sampling\n# stratified_df = pd.DataFrame()\n\n# comprehensive_imputed_df[[\"CUST_ACCOUNT_NUMBER\", \"DAYS_TO_CHURN\"]]\n\n# label_count=1\n# for i in range(365, 1461, 365):\n#     XX = comprehensive_imputed_df[((i-365) < comprehensive_imputed_df[\"DAYS_TO_CHURN\"]) & (comprehensive_imputed_df[\"DAYS_TO_CHURN\"] <= i)]\n#     #XX[\"SAMPLE\"] = label_count\n#     stratified_df = pd.concat([stratified_df, XX], axis=0, ignore_index=True)\n#     #label_count+=1\n#     #print(\"For \", (i-365)+1, \" To \", i , \" Total Counts = \", XX.shape[0])\n#     if i==1460:\n#         XX = comprehensive_imputed_df[1460 < comprehensive_imputed_df[\"DAYS_TO_CHURN\"]]\n#         XX[\"SAMPLE\"] = label_count\n#         stratified_df = pd.concat([stratified_df, XX], axis=0, ignore_index=True)\n#         #print(\"For \", 1460+1, \" To upper limit Total Counts = \", (1460 < data[\"DAYS_TO_CHURN\"]).sum())\n\n\n#stratified_df[\"LOG_OF_MONTHS_TO_CHURN\"] = np.log(stratified_df[\"LIFESPAN_MONTHS\"])\n\n# Stratified Sampling\n#features = stratified_df.drop([\"CUST_ACCOUNT_NUMBER\",\"LIFESPAN_MONTHS\",\"DAYS_TO_CHURN\", \"SAMPLE\"], axis=1)\n# features = comprehensive_imputed_df.drop([\"CUST_ACCOUNT_NUMBER\",\"LIFESPAN_MONTHS\",\"DAYS_TO_CHURN\", \"DAYS_REMAINING\"], axis=1) # Let cluster be in training\n# y = comprehensive_imputed_df[\"DAYS_REMAINING\"]\n\n# 20/80 Split\n# X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.2, random_state=42, shuffle=True)#, stratify=y)\n#y_train = X_train[\"LOG_OF_MONTHS_TO_CHURN\"]\n#y_test = X_test[\"LOG_OF_MONTHS_TO_CHURN\"]\n\n#X_train.drop(\"LOG_OF_MONTHS_TO_CHURN\", axis=1, inplace=True)\n#X_test.drop(\"LOG_OF_MONTHS_TO_CHURN\", axis=1, inplace=True)\n\n# Training\n\nparam_grid = {\"learning_rate\": Real(0.01, 1.0, \"uniform\"),\n             \"max_depth\": Integer(2, 12),\n             \"subsample\": Real(0.1, 1.0, \"uniform\"),\n             \"colsample_bytree\": Real(0.1, 1.0, \"uniform\"), # subsample ratio of columns by tree\n             \"reg_lambda\": Real(1e-9, 100., \"uniform\"), # L2 regularization\n             \"reg_alpha\": Real(1e-9, 100., \"uniform\"), # L1 regularization\n             \"n_estimators\": Integer(20, 5000)\n}\n\nxgb_model = xgb.XGBRegressor(tree_method=\"gpu_hist\", random_state=10)\n\nbayes_search = BayesSearchCV(\n        estimator=xgb_model, \n        search_spaces=param_grid, \n        scoring=\"neg_mean_squared_error\", \n        cv=5,\n        n_iter=50,\n        n_jobs=-1,\n        verbose=3,\n        random_state=0)\n\nprint(\"Training Starts\")\n\nbayes_search.fit(train_X, train_y)\n\nprint(\"Training Finishes\")\n\nbest_models = bayes_search.best_estimator_\n\n#y_pred = best_models.predict(X_test)\n#rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n\nimport time\nimport datetime\nts = time.time()\ntime_of_day = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d::%H:%M:%S')\n\nprint(\"Writing model into Staging Archive\")\n#import_dir = sys._xoptions.get(\"snowflake_import_directory\")\nmodel_file = os.path.join(\"/tmp\", \"xgb.joblib.gz\")\ndump(best_models, model_file)\nsession.file.put(model_file, \"@PS_DOCUWARE_CHURN/ps_docuware_churn_model\",overwrite=True)\n\nprint(\"Writing model into Staging\")\nmodel_file = os.path.join(\"/tmp\", \"xgb.\"+str(time_of_day)+\".joblib.gz\")\ndump(best_models, model_file)\nsession.file.put(model_file, \"@PS_DOCUWARE_CHURN/ps_docuware_churn_model/archive\",overwrite=True)\n\nprint(\"Successfuly trained the model and stored\" )   \n#except Exception as e: \n#print(\"FAIL(Post Processing)!\" + \" Error: \" + str(e))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c91bc795-c622-4afe-bf47-4e7d140c1ab1",
   "metadata": {
    "name": "cell96",
    "collapsed": false
   },
   "source": "# Test for churned customers"
  },
  {
   "cell_type": "code",
   "id": "febd5c75-05ec-4783-8c78-d04089f7d09e",
   "metadata": {
    "language": "python",
    "name": "cell121"
   },
   "outputs": [],
   "source": "comprehensive_imputed_df.loc[test_X.index,\"CUST_ACCOUNT_NUMBER\"].astype(str).unique()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46f0399f-0026-4764-b494-5446a6e34295",
   "metadata": {
    "language": "python",
    "name": "cell122"
   },
   "outputs": [],
   "source": "last_rows_X_test = X_test_all_cols[ X_test_all_cols[\"DAYS_REMAINING\"] == 0]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc161da3-935c-4ff4-a764-2209c457837c",
   "metadata": {
    "language": "python",
    "name": "cell123"
   },
   "outputs": [],
   "source": "last_rows_X_test[[\"CUST_ACCOUNT_NUMBER\", \"DAYS_REMAINING\"]].index",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9d5826c-61a7-41e8-9392-eb86fb00305f",
   "metadata": {
    "language": "python",
    "name": "cell124"
   },
   "outputs": [],
   "source": "test_25_X = last_rows_X_test.drop([\"CUST_ACCOUNT_NUMBER\",\"LIFESPAN_MONTHS\",\"DAYS_TO_CHURN\", \"DAYS_REMAINING\"], axis=1) # Let cluster be in training\ntest_25_y = last_rows_X_test[\"DAYS_REMAINING\"]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ed2fe790-8dde-44df-8829-4b046271d209",
   "metadata": {
    "language": "python",
    "name": "cell125"
   },
   "outputs": [],
   "source": "test_25_X.shape, test_25_y.shape",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1dc40cbf-2a96-4a5c-9ee4-c24dad56a45a",
   "metadata": {
    "language": "python",
    "name": "cell126"
   },
   "outputs": [],
   "source": "test_25_y",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56d9f2ce-d4b0-4fc7-b1d5-aff2fd7b9b47",
   "metadata": {
    "language": "python",
    "name": "cell127"
   },
   "outputs": [],
   "source": "# Inferencing\ny_pred = best_models.predict(test_25_X)\nremaining_months = y_pred/30\nprint(remaining_months.shape)\n\nactual_remaining = test_25_y/30\nfinal_test_predicted_df = pd.DataFrame({\n    \"CUST_ACCOUNT_NUMBER\": last_rows_X_test.loc[test_25_X.index,\"CUST_ACCOUNT_NUMBER\"].astype(str),\n    \"PREDICTED_MONTHS_REMAINING\": remaining_months,\n    \"ACTUAL_MONTH_REMAINING\": actual_remaining,\n    \"RESIDUAL\": abs(remaining_months-actual_remaining)\n})\n\nfinal_test_predicted_df.head(50)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "10bd622c-e196-44e8-aca1-3c9c03d9ca70",
   "metadata": {
    "language": "python",
    "name": "cell95",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Load Training data\n# comprehensive_imputed_df = session.sql(\"SELECT * FROM RUS_AIML.PS_DOCUWARE_IMPUTED_DATA\").to_pandas()\n# comprehensive_imputed_df = comprehensive_imputed_df.replace([np.inf, -np.inf], 0)\n# comprehensive_imputed_df = comprehensive_imputed_df.fillna(0)\n\n# for col in comprehensive_imputed_df.columns:\n#     comprehensive_imputed_df[col] = pd.to_numeric(comprehensive_imputed_df[col], errors=\"coerce\").astype(float)\n\n# Stratified Sampling\n#features = stratified_df.drop([\"CUST_ACCOUNT_NUMBER\",\"LIFESPAN_MONTHS\",\"DAYS_TO_CHURN\", \"SAMPLE\"], axis=1)\n# features = comprehensive_imputed_df.drop([\"CUST_ACCOUNT_NUMBER\",\"LIFESPAN_MONTHS\",\"DAYS_TO_CHURN\", \"DAYS_REMAINING\"], axis=1) # Let cluster be in training\n# y = comprehensive_imputed_df[\"DAYS_REMAINING\"]\n\n# # 20/80 Split\n# X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.2, random_state=42, shuffle=True)#, stratify=y)\n\n# # Load model_file\n# model_file = os.path.join('/tmp', 'xgb.joblib.gz')\n# session.file.get(\"@PS_DOCUWARE_CHURN/ps_docuware_churn_model/xgb.joblib.gz\", \"/tmp\")\n# best_models = load(model_file)\n\n# # Split XTrain,Ytrain\n\n\n# Inferencing\ny_pred = best_models.predict(test_X)\nremaining_months = y_pred/30\nprint(remaining_months.shape)\n\nactual_remaining = test_y/30\nfinal_test_predicted_df = pd.DataFrame({\n    \"CUST_ACCOUNT_NUMBER\": comprehensive_imputed_df.loc[test_X.index,\"CUST_ACCOUNT_NUMBER\"].astype(str),\n    \"PREDICTED_MONTHS_REMAINING\": remaining_months,\n    \"ACTUAL_MONTH_REMAINING\": actual_remaining,\n    \"RESIDUAL\": abs(remaining_months-actual_remaining)\n})\n\nfinal_test_predicted_df.head(796)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c7c646ca-d8c6-45cb-b99d-c370357531fb",
   "metadata": {
    "language": "python",
    "name": "cell97"
   },
   "outputs": [],
   "source": "# Feature Importance Scores\nimportance = best_model.get_booster().get_score(importance_type='gain')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c092e219-ddd1-4d17-952d-c3fc47d355de",
   "metadata": {
    "language": "python",
    "name": "cell98"
   },
   "outputs": [],
   "source": "importance_df = pd.DataFrame(importance.items(), columns=['Feature', 'Score'])\nimportance_df.head(287)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9637c57c-9bfa-4f1a-a5b3-8e18869f28ff",
   "metadata": {
    "name": "cell58"
   },
   "source": "# Plot of Important Features ranked by XGBoost algorithm"
  },
  {
   "cell_type": "code",
   "id": "90aef8b7-c211-430c-9e03-23d6fc29b8b7",
   "metadata": {
    "language": "python",
    "name": "cell44"
   },
   "outputs": [],
   "source": "plt.figure(figsize=(40,30))\nxgb.plot_importance(best_model, max_num_features=20)\n#plt.savefig('C:/My Documents/work/Projects/Short Term Goal 2 - Customer Churn Prediction/ML Modeling Phase/Overall_Model_Importance_16Apr_2025.png')\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4be680d4-2c65-4a6e-be3b-1066d7447177",
   "metadata": {
    "name": "cell59"
   },
   "source": "# Data extrcation of Live Customers for Inferencing"
  },
  {
   "cell_type": "code",
   "id": "76ffd01c-da72-484d-8cde-49b3cad55eaf",
   "metadata": {
    "language": "python",
    "name": "Sproc_5",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#Fetching all the different views\npay_df = session.sql(\"SELECT * FROM RUS_AIML.PS_DOCUWARE_PAYMENTS_V\").to_pandas()\nrev_df = session.sql(\"SELECT * FROM RUS_AIML.PS_DOCUWARE_REVENUE_V\").to_pandas()\ntrx_df = session.sql(\"SELECT * FROM RUS_AIML.PS_DOCUWARE_TRX_V\").to_pandas()\ncontracts_sub_df = session.sql(\"SELECT * FROM RUS_AIML.PS_DOCUWARE_CONTRACTS_SUBLINE_V\").to_pandas()\nrenewals_df = session.sql(\"SELECT * FROM RUS_AIML.PS_DOCUWARE_RENEWALS_V\").to_pandas()\nl1_cust_df =  session.sql(\"SELECT * FROM RUS_AIML.PS_DOCUWARE_L1_CUST_V\").to_pandas()\ndnb_risk_df = session.sql(\"SELECT * FROM RUS_AIML.DNB_RISK_BREAKDOWN_V\").to_pandas()\nusage_latest = session.sql(\"SELECT * FROM RUS_AIML.DOCUWARE_USAGE_JAPAN_V1_LATEST_V\").to_pandas()\n\n# Removing duplicates from all the pandas dataframes\npay_df = pay_df.drop_duplicates()\nrev_df = rev_df.drop_duplicates()\ntrx_df = trx_df.drop_duplicates()\ncontracts_sub_df = contracts_sub_df.drop_duplicates()\nrenewals_df = renewals_df.drop_duplicates()\nl1_cust_df = l1_cust_df.drop_duplicates()\ndnb_risk_df = dnb_risk_df.drop_duplicates()\nusage_latest = usage_latest.drop_duplicates()\n\n# Selecting active customers\n\nl1_cust_active = l1_cust_df[l1_cust_df[\"CHURNED_FLAG\"]==False]\n\npay_df_active = pay_df.merge(l1_cust_active[[\"CUST_ACCOUNT_NUMBER\"]], left_on = \"CUSTOMER_NO\", right_on = \"CUST_ACCOUNT_NUMBER\", how=\"inner\")\npay_df_active = pay_df_active.drop(\"CUSTOMER_NO\", axis=1)\npay_df_active[\"MONTH\"] = pd.to_datetime(pay_df_active[\"RECEIPT_DATE\"]).dt.to_period(\"M\").dt.to_timestamp()\n\nrev_df_active = rev_df.merge(l1_cust_active[[\"CUST_ACCOUNT_NUMBER\"]], on = \"CUST_ACCOUNT_NUMBER\", how=\"inner\")\nrev_df_active[\"MONTH\"] = pd.to_datetime(rev_df_active[\"DATE_INVOICE_GL_DATE\"]).dt.to_period(\"M\").dt.to_timestamp()\n\ntrx_df_active = trx_df.merge(l1_cust_active[[\"CUST_ACCOUNT_NUMBER\"]], left_on = \"ACCOUNT_NUMBER\", right_on = \"CUST_ACCOUNT_NUMBER\", how=\"inner\")\ntrx_df_active = trx_df_active.drop(\"ACCOUNT_NUMBER\", axis=1)\ntrx_df_active[\"MONTH\"] = pd.to_datetime(trx_df_active[\"TRX_DATE\"]).dt.to_period(\"M\").dt.to_timestamp()\n\np_r_t_merged = pay_df_active.merge(rev_df_active, on = [\"CUST_ACCOUNT_NUMBER\",\"MONTH\"], how=\"outer\").merge(trx_df_active, on = [\"CUST_ACCOUNT_NUMBER\",\"MONTH\"], how=\"outer\")\n\n#Contracts Subline\ncontracts_sub_df[\"SLINE_START_DATE\"] = pd.to_datetime(contracts_sub_df[\"SLINE_START_DATE\"])\ncontracts_sub_df[\"SLINE_END_DATE\"] = pd.to_datetime(contracts_sub_df[\"SLINE_END_DATE\"])\ncontracts_sub_df[\"SUB_START_MONTH\"] = pd.to_datetime(contracts_sub_df[\"SLINE_START_DATE\"]).dt.to_period(\"M\").dt.to_timestamp()\ncontracts_sub_df[\"SUB_END_MONTH\"] = pd.to_datetime(contracts_sub_df[\"SLINE_END_DATE\"]).dt.to_period(\"M\").dt.to_timestamp()\ncontracts_sub_df[\"SUB_EARLIEST_MONTH\"] = contracts_sub_df.groupby(\"CUST_ACCOUNT_NUMBER\")[\"SUB_START_MONTH\"].transform(\"min\")\ncontracts_sub_df[\"SUB_LATEST_MONTH\"] = contracts_sub_df.groupby(\"CUST_ACCOUNT_NUMBER\")[\"SUB_END_MONTH\"].transform(\"max\")\ncontracts_sub_df_active = contracts_sub_df.merge(l1_cust_active[[\"CUST_ACCOUNT_NUMBER\"]], on = \"CUST_ACCOUNT_NUMBER\", how=\"inner\")\n\ncontracts_sub_df_active = contracts_sub_df_active.drop_duplicates()\n\n# Renewals\ncols_to_str = [\"BILLTOCUSTOMERNUMBER\",\"SHIPTOCUSTNUM\"]\nrenewals_df[cols_to_str] = renewals_df[cols_to_str].astype(\"Int64\").astype(str)\n\nrenewals_df_active_1 = renewals_df.merge(l1_cust_active[[\"CUST_ACCOUNT_NUMBER\"]], left_on = \"SHIPTOCUSTNUM\", right_on = \"CUST_ACCOUNT_NUMBER\",  how=\"left\")\n\nrenewals_df_active_2 = renewals_df_active_1.merge(l1_cust_active[[\"CUST_ACCOUNT_NUMBER\"]], left_on = \"BILLTOCUSTOMERNUMBER\", right_on = \"CUST_ACCOUNT_NUMBER\",  how=\"left\", suffixes = (\"_1\", \"_2\"))\nrenewals_df_active_2[\"CUST_ACCOUNT_NUMBER\"] = renewals_df_active_2[\"CUST_ACCOUNT_NUMBER_1\"].fillna(renewals_df_active_2[\"CUST_ACCOUNT_NUMBER_2\"])\nrenewals_df_active_2 = renewals_df_active_2.drop([\"BILLTOCUSTOMERNUMBER\", \"SHIPTOCUSTNUM\",\"CUST_ACCOUNT_NUMBER_1\",\"CUST_ACCOUNT_NUMBER_2\"], axis=1)\nrenewals_df_active_2 = renewals_df_active_2.dropna()\n\nrenewals_df_active_2[\"STARTDATECOVERAGE\"] =  pd.to_datetime(renewals_df_active_2[\"STARTDATECOVERAGE\"])\nrenewals_df_active_2[\"RENEWALS_START_MONTH\"] = pd.to_datetime(renewals_df_active_2[\"STARTDATECOVERAGE\"]).dt.to_period(\"M\").dt.to_timestamp()\nrenewals_df_active_2[\"RENEWALS_END_MONTH\"] = pd.to_datetime(renewals_df_active_2[\"CONTRACT_END_DATE\"]).dt.to_period(\"M\").dt.to_timestamp()\nrenewals_df_active_2[\"RENEWALS_EARLIEST_MONTH\"] = renewals_df_active_2.groupby(renewals_df_active_2[\"CUST_ACCOUNT_NUMBER\"])[\"RENEWALS_START_MONTH\"].transform(\"min\")\nrenewals_df_active_2[\"RENEWALS_LATEST_MONTH\"] = renewals_df_active_2.groupby(renewals_df_active_2[\"CUST_ACCOUNT_NUMBER\"])[\"RENEWALS_END_MONTH\"].transform(\"max\")\n\n#DNB Risk Breakdown\ndnb_risk_df[\"ACCOUNT_NUMBER\"] = dnb_risk_df[\"ACCOUNT_NUMBER\"].astype(\"Int64\").astype(str)\ndnb_risk_df_active = dnb_risk_df.merge(l1_cust_active[[\"CUST_ACCOUNT_NUMBER\"]], left_on = \"ACCOUNT_NUMBER\", right_on = \"CUST_ACCOUNT_NUMBER\",  how=\"inner\")\ndnb_risk_df_active = dnb_risk_df_active.drop(\"ACCOUNT_NUMBER\", axis=1)\n\nusage_latest[\"YYYYWK_Transformed\"] = pd.to_datetime(usage_latest[\"YYYYWK\"].apply(convert_yyyywk_to_date), errors = \"coerce\")\n\n#Usage Japan Latest\nusage_latest[\"CUST_ACCOUNT_NUMBER\"] = usage_latest[\"CUST_ACCOUNT_NUMBER\"].astype(\"Int64\").astype(str)\n\nusage_latest[\"YYYYWK_MONTH\"] = pd.to_datetime(usage_latest[\"YYYYWK_Transformed\"]).dt.to_period(\"M\").dt.to_timestamp()\n\nusage_latest_active = usage_latest.merge(l1_cust_active[[\"CUST_ACCOUNT_NUMBER\"]], on=\"CUST_ACCOUNT_NUMBER\",  how=\"inner\")\n\n# Merging all the active customers data frames i.e. Payments, Revenue, Transactions, contracts, contracts subline, contracts topline, renewals, snow inc, tech survey, loyalty survey, dnb risk and usage latest\n\nmerged_1 = p_r_t_merged.merge(contracts_sub_df_active, left_on = [\"CUST_ACCOUNT_NUMBER\", \"MONTH\"], right_on = [\"CUST_ACCOUNT_NUMBER\", \"SUB_START_MONTH\"], how=\"outer\")\n\nmerged_1[\"MONTH\"] = merged_1[\"MONTH\"].fillna(merged_1[\"SUB_START_MONTH\"])\nmerged_1 = merged_1.drop(\"SUB_START_MONTH\", axis=1)\nmerged_2 = merged_1.merge(renewals_df_active_2, left_on = [\"CUST_ACCOUNT_NUMBER\",\"MONTH\"], right_on = [\"CUST_ACCOUNT_NUMBER\",\"RENEWALS_START_MONTH\"], how=\"outer\")\nmerged_2[\"MONTH\"] = merged_2[\"MONTH\"].fillna(merged_2[\"RENEWALS_START_MONTH\"])\nmerged_2 = merged_2.drop(\"RENEWALS_START_MONTH\", axis=1)\n\nmerged_3 = merged_2.merge(dnb_risk_df_active, on=\"CUST_ACCOUNT_NUMBER\", how=\"left\")\n\nmerged_4 = merged_3.merge(usage_latest_active, left_on= [\"CUST_ACCOUNT_NUMBER\",\"MONTH\"], right_on = [\"CUST_ACCOUNT_NUMBER\", \"YYYYWK_MONTH\"], how=\"outer\")\nmerged_4[\"MONTH\"] = merged_4[\"MONTH\"].fillna(merged_4[\"YYYYWK_MONTH\"])\n\nto_drop_2 = [\"CONTRACT_NUMBER\", \"CUST_PARTY_NAME\", \"CUSTOMER_NAME\", \"CONTRACT_END\",\"JAROWINKLER_SIMILARITY(A.CUST_PARTY_NAME, B.CUSTOMER_NAME)\"]\n\nmerged_4 = merged_4.drop(to_drop_2, axis=1)\n\nmerged_5 = merged_4.merge(l1_cust_active[[\"CUST_ACCOUNT_NUMBER\", \"CHURNED_FLAG\"]], on = \"CUST_ACCOUNT_NUMBER\", how=\"inner\")\nmerged_5[\"EARLIEST_DATE\"] = merged_5[[\"RECEIPT_DATE\", \"DATE_INVOICE_GL_DATE\", \"TRX_DATE\", \"SLINE_START_DATE\", \"STARTDATECOVERAGE\"]].min(axis=1)\nmerged_5[\"FINAL_EARLIEST_DATE\"] = merged_5.groupby(\"CUST_ACCOUNT_NUMBER\")[\"EARLIEST_DATE\"].transform(\"min\")\n\nto_drop_3 = [\"RECEIPT_DATE\", \"DATE_INVOICE_GL_DATE\", \"TRX_DATE\", \"SLINE_START_DATE\", \"SLINE_END_DATE\", \"SUB_END_MONTH\",\"SLINE_STATUS\", \"SUB_EARLIEST_MONTH\", \"SUB_LATEST_MONTH\", \"STARTDATECOVERAGE\", \"CONTRACT_END_DATE\", \"RENEWALS_END_MONTH\", \"RENEWALS_EARLIEST_MONTH\",\"RENEWALS_LATEST_MONTH\", \"YYYYWK_MONTH\", \"PERIOD\", \"CHURNED_FLAG\", \"EARLIEST_DATE\"]\n\nmerged_5 = merged_5.drop(to_drop_3, axis=1)\n\ndate_col = [\"MONTH\",\"YYYYWK_Transformed\", \"FINAL_EARLIEST_DATE\"]\n\nmerged_5[date_col] = merged_5[date_col].astype(str)\n\nmerged_5 = merged_5.drop_duplicates()\n\nsession.write_pandas(merged_5, \"PS_DOCUWARE_RAW_DATA_PREDICTION\", auto_create_table=True, overwrite = True)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a4ac6f27-4c13-45ca-b6bc-38c9a8442d47",
   "metadata": {
    "name": "cell60"
   },
   "source": "# Data Preprocessing of Live customers"
  },
  {
   "cell_type": "code",
   "id": "49680e97-477c-480b-aa64-18fa106a1400",
   "metadata": {
    "language": "python",
    "name": "Sproc_6",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#Fetching from previous SPROC output table\nraw_df_active = session.sql(\"SELECT * FROM RUS_AIML.PS_DOCUWARE_RAW_DATA_PREDICTION\").to_pandas()\n\nnon_ts_numeric_cols = [\"PROBABILITY_OF_DELINQUENCY\", \"RICOH_CUSTOM_RISK_MODEL\"]\nnon_ts_categorical_cols = [\"OVERALL_BUSINESS_RISK\", \"PAYMENT_RISK_TRIPLE_A_RATING\", \"CONTRACT_LINE_ITEMS\"]\ncolumns_to_be_processed_later = non_ts_numeric_cols + non_ts_categorical_cols + [\"CUST_ACCOUNT_NUMBER\"]\nfinalized_df_ohe_to_process = raw_df_active.groupby(\"CUST_ACCOUNT_NUMBER\")[columns_to_be_processed_later].first()\n\n\n# Imputation for Non Time Series columns\npofd_median = finalized_df_ohe_to_process[\"PROBABILITY_OF_DELINQUENCY\"].median()\nfinalized_df_ohe_to_process[\"PROBABILITY_OF_DELINQUENCY\"] = finalized_df_ohe_to_process[\"PROBABILITY_OF_DELINQUENCY\"].apply(lambda x:   float(pofd_median) if np.isnan(x) else x)\n\nrcrm_mode = finalized_df_ohe_to_process[\"RICOH_CUSTOM_RISK_MODEL\"].mode()\nfinalized_df_ohe_to_process[\"RICOH_CUSTOM_RISK_MODEL\"] = finalized_df_ohe_to_process[\"RICOH_CUSTOM_RISK_MODEL\"].apply(lambda x: float(rcrm_mode) if np.isnan(x) else x)\n\n# One Hot Encoding for Categorical variables OVERALL_BUSINESS_RISK and PAYMENT_RISK_TRIPLE_A_RATING\nfinalized_df_ohe_to_process = finalized_df_ohe_to_process.reset_index(drop=True)\n\ncategorical_cols = [\"OVERALL_BUSINESS_RISK\", \"PAYMENT_RISK_TRIPLE_A_RATING\"]\nfill_value = \"UNK\"\n\nfor col in categorical_cols:\n    finalized_df_ohe_to_process[col].fillna(fill_value, inplace=True)\n    finalized_df_ohe_to_process[col] = finalized_df_ohe_to_process[col].str.replace(\" \", \"_\", regex=False)\n    \n    if col == \"OVERALL_BUSINESS_RISK\":\n        col_abreviation = \"obr_\"\n    else:\n        col_abreviation = \"prtar_\"\n        \n    le_ohe = LabelEncoder()\n    ohe = OneHotEncoder(handle_unknown = \"ignore\")\n    enc_train = le_ohe.fit_transform(finalized_df_ohe_to_process[col]).reshape(finalized_df_ohe_to_process.shape[0],1)\n    ohe_train = ohe.fit_transform(enc_train)\n    le_ohe_name_mapping = dict(zip(le_ohe.classes_, le_ohe.transform(le_ohe.classes_)))\n    \n    enc_train = finalized_df_ohe_to_process[col].map(le_ohe_name_mapping).ravel().reshape(-1,1)\n    enc_train[np.isnan(enc_train)] = 9999\n\n    cols = [col_abreviation + str(x) for x in le_ohe_name_mapping.keys()]\n    finalized_df_ohe_to_process = pd.concat([finalized_df_ohe_to_process.reset_index(), pd.DataFrame.sparse.from_spmatrix(ohe_train, columns = cols)], axis = 1).drop([\"index\"], axis=1)\n    finalized_df_ohe_to_process.drop([col], axis = 1, inplace=True)\n\ncolumns_to_be_droped = non_ts_categorical_cols+non_ts_numeric_cols\nraw_df_active.drop(columns_to_be_droped, axis=1, inplace=True)\n\n# Target Encoding for CONTRACT_LINE_ITEMS\nfinalized_df_ohe_to_process[\"CONTRACT_LINE_ITEMS\"].fillna(\"NA\", inplace=True)\nfinalized_df_ohe_to_process[\"CONTRACT_LINE_ITEMS\"] = finalized_df_ohe_to_process[\"CONTRACT_LINE_ITEMS\"].str.replace(r\"\\\\d+x \", \"\", regex=True)\n\nfor i, row in finalized_df_ohe_to_process.iterrows():\n    t = row[\"CONTRACT_LINE_ITEMS\"]\n    arr = t.split(\"-\")\n    arr = [x.strip() for x in arr]\n    arr_s = sorted(arr)\n    key = \"-\".join([s for s in arr_s])\n    finalized_df_ohe_to_process.loc[i, \"CONTRACT_LINE_ITEMS\"] = key\n\nstage_path = \"@PS_DOCUWARE_CHURN/ps_docuware_target_encoder_object\"\n\nX_train = finalized_df_ohe_to_process.copy()\n\nimport_dir = sys._xoptions.get(\"snowflake_import_directory\")\nenc_file = os.path.join('/tmp', 'ENC_CURRENT.joblib.gz')\nsession.file.get(\"@PS_DOCUWARE_CHURN/ps_docuware_target_encoder_object/ENC_CURRENT.joblib.gz\", '/tmp')\nenc = load(enc_file)\n\nX_train = X_train.reindex(columns = enc.feature_names_in_, fill_value=0)\nX_train_encoded = enc.transform(X_train)\n\n# Imputation for Time Series columns\nts_columns = [\"CUST_ACCOUNT_NUMBER\", \"YYYYWK\", \"DOCUMENTS_OPENED\", \"USED_STORAGE__MB\", \"INVOICE_REVLINE_TOTAL\", \"ORIGINAL_AMOUNT_DUE\", \"FUNCTIONAL_AMOUNT\"]\nraw_df_active[\"transformed_YYYYWK\"] = raw_df_active[\"MONTH\"].apply(convert_date_to_yyyywk)\n#Impute missing YYYYWK with equivalent MONTH\nraw_df_active[\"YYYYWK\"].fillna(raw_df_active[\"transformed_YYYYWK\"], inplace=True)\nraw_df_active.drop(\"transformed_YYYYWK\", axis=1, inplace=True)\nts_df = raw_df_active[ts_columns]\nts_df = ts_df[ts_df['YYYYWK'].notna()]\nts_df['YYYYWK'] = ts_df['YYYYWK'].astype(int)\nts_df['CUST_ACCOUNT_NUMBER'] = ts_df['CUST_ACCOUNT_NUMBER'].astype(int)\nts_df.rename(columns={'USED_STORAGE__MB':'USED_STORAGE_MB'},inplace=True)\nts_df_sorted = ts_df.sort_values(['CUST_ACCOUNT_NUMBER','YYYYWK']).drop_duplicates()\n\nonly_features = ts_df_sorted.copy()\nonly_features = only_features.fillna(0)\n\n#ts_comprehensive_df = session.sql(\"SELECT * FROM RAC_RAPID_DEV.RUS_AIML.PS_DOCUWARE_PREDICTION_TS_DF_ONLY\").to_pandas()\n#training_set_ts_df = session.sql(\"SELECT * FROM RAC_RAPID_DEV.RUS_AIML.PS_DOCUWARE_ONLY_TS_DF\").to_pandas()\n\n\n#ts_comprehensive_df_active = time_series_ts_fresh_features(only_features)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4bb621d1-afd2-44cc-9de1-ec7e0786bec3",
   "metadata": {
    "name": "cell61",
    "collapsed": false
   },
   "source": "# Store Live Customers' TimeSeries data before TSFRESH into DB"
  },
  {
   "cell_type": "code",
   "id": "8481c003-c881-4eec-9b30-9515250eb028",
   "metadata": {
    "language": "python",
    "name": "cell45"
   },
   "outputs": [],
   "source": "session.write_pandas(only_features, \"PS_DOCUWARE_LIVE_RAW_DATA_BEFORE_TSFRESH\", auto_create_table=True, overwrite = True)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "09c8fd21-577e-4549-8c5e-c5fdaca5687d",
   "metadata": {
    "name": "cell62"
   },
   "source": "# Feature Engineering using TSFRESH"
  },
  {
   "cell_type": "code",
   "id": "7db422db-f6ff-4d67-8967-6770b7dc66ba",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": "ts_comprehensive_df_active = engineer_timeseries_cols_using_tsfresh_for_live_customers(only_features)\nts_comprehensive_df_active = ts_comprehensive_df_active.replace([np.inf, -np.inf], 0)\nts_comprehensive_df_active = ts_comprehensive_df_active.fillna(0)\ntraining_set_ts_df = ts_comprehensive_df_active.copy()\nsame_features = training_set_ts_df.columns",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "670f2357-8f51-4c82-a862-470fec01fe6f",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "#same_features\n#ts_comprehensive_df_active = fts_comprehensive_df_active",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "35cebbbe-fe7d-4c2d-b64c-234744a9d420",
   "metadata": {
    "name": "cell63"
   },
   "source": "# Store LIVE Customers' TSFRESH time series generated features"
  },
  {
   "cell_type": "code",
   "id": "89a7db38-0860-4e80-a0e1-999bdeeda4c9",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": "ts_comprehensive_df_active.shape\nsession.write_pandas(ts_comprehensive_df_active, \"PS_DOCUWARE_LIVE_CUSTOMER_FEATURES\", auto_create_table=True, overwrite = True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7810892b-f161-4fbe-b25f-e1cafdd395d5",
   "metadata": {
    "language": "python",
    "name": "cell40"
   },
   "outputs": [],
   "source": "# #comprehensive_imputed_df_active['FINAL_EARLIEST_DATE']\n# temp = raw_df_active[['CUST_ACCOUNT_NUMBER', 'FINAL_EARLIEST_DATE']].drop_duplicates()\n# temp.shape, ts_comprehensive_df_active.shape, X_train_encoded.shape",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "64c93071-8cd9-4bda-addd-50c5faf6f77f",
   "metadata": {
    "name": "cell64"
   },
   "source": "# Data preprocessing to prepare it for inferencing "
  },
  {
   "cell_type": "code",
   "id": "67f6a561-2168-4d86-bcf6-d544240c0c8e",
   "metadata": {
    "language": "python",
    "name": "cell8",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "ts_comprehensive_df_active = ts_comprehensive_df_active.reindex(columns=same_features, fill_value=0)\nts_comprehensive_df_active[\"CUST_ACCOUNT_NUMBER\"] = ts_comprehensive_df_active[\"CUST_ACCOUNT_NUMBER\"].astype(int)\n\nX_train_encoded[\"CUST_ACCOUNT_NUMBER\"] = X_train_encoded[\"CUST_ACCOUNT_NUMBER\"].astype(int)\ndf_earliest_date = raw_df_active[['CUST_ACCOUNT_NUMBER', 'FINAL_EARLIEST_DATE']].drop_duplicates()\ndf_earliest_date[\"CUST_ACCOUNT_NUMBER\"] = df_earliest_date[\"CUST_ACCOUNT_NUMBER\"].astype(int)\n\ncomprehensive_imputed_df_active = pd.merge(ts_comprehensive_df_active, X_train_encoded, on=\"CUST_ACCOUNT_NUMBER\", how=\"inner\")\ncomprehensive_imputed_df_active = pd.merge(comprehensive_imputed_df_active, df_earliest_date, on=\"CUST_ACCOUNT_NUMBER\", how=\"inner\")\ncomprehensive_imputed_df_active['CREATION_DATE'] = pd.Timestamp.today().date()\n\n#comprehensive_imputed_df_active['DAYS_ELAPSED_TILL_DATE'] = (pd.to_datetime(comprehensive_imputed_df_active['CREATION_DATE']) - pd.to_datetime(comprehensive_imputed_df_active['FINAL_EARLIEST_DATE'])).dt.days\n\n#stratified_df_active = pd.DataFrame()\n\n# Get CLUSTER Information in the dataframe\n#label_count=1\n# for i in range(365, 1461, 365):\n#     XX = comprehensive_imputed_df_active[((i-365) < comprehensive_imputed_df_active[\"DAYS_ELAPSED_TILL_DATE\"]) & (comprehensive_imputed_df_active[\"DAYS_ELAPSED_TILL_DATE\"] <= i)]\n#     #XX[\"SAMPLE\"] = label_count\n#     stratified_df_active = pd.concat([stratified_df_active, XX], axis=0, ignore_index=True)\n#     #label_count+=1\n   \n#     if i==1460:\n#         XX = comprehensive_imputed_df_active[1460 < comprehensive_imputed_df_active[\"DAYS_ELAPSED_TILL_DATE\"]]\n#         #XX[\"SAMPLE\"] = label_count\n#         stratified_df_active = pd.concat([stratified_df_active, XX], axis=0, ignore_index=True)\n\ncomprehensive_imputed_df_active_with_cluster = comprehensive_imputed_df_active.copy()\n# comprehensive_imputed_df_active_with_cluster = stratified_df_active.copy()\n\nfor col in comprehensive_imputed_df_active_with_cluster.columns:\n    if pd.api.types.is_sparse(comprehensive_imputed_df_active_with_cluster[col]):\n        comprehensive_imputed_df_active_with_cluster[col] = comprehensive_imputed_df_active_with_cluster[col].sparse.to_dense()\n\n#import_dir = sys._xoptions.get(\"snowflake_import_directory\")        \nmodel_file = os.path.join('/tmp', 'xgb.joblib.gz')\nsession.file.get(\"@PS_DOCUWARE_CHURN/ps_docuware_churn_model/xgb.joblib.gz\", \"/tmp\")\nbest_model = load(model_file)\n\n#input_features = comprehensive_imputed_df_active_with_cluster.drop([\"CUST_ACCOUNT_NUMBER\", \"CREATION_DATE\", \"DAYS_ELAPSED_TILL_DATE\"], axis=1)\ninput_features = comprehensive_imputed_df_active_with_cluster.drop([\"CUST_ACCOUNT_NUMBER\", \"CREATION_DATE\"], axis=1)\n\ninput_features = convert_to_float(input_features)\ninput_features = input_features.reindex(columns=best_model.feature_names_in_, fill_value=0)\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0a13b7e7-bb4b-4097-af08-1976b7c9b8dd",
   "metadata": {
    "name": "cell68"
   },
   "source": "# Store features of Live Customers into Staging"
  },
  {
   "cell_type": "code",
   "id": "9ccf4a66-faad-4fd9-ad63-2c7777224f74",
   "metadata": {
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": "df_live_customers = session.create_dataframe(input_features)\n#df_live_customers.write.copy_into_location(\"@PS_DOCUWARE_CHURN/df_live_customers_21Apr2025.csv\", file_format_type=\"csv\", format_type_options={\"COMPRESSION\": \"NONE\"}, header=True, overwrite=True )\n        ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "20271eb5-2e02-4721-8a07-b7960817dda3",
   "metadata": {
    "name": "cell69"
   },
   "source": "# Store features of Live Customers into DB"
  },
  {
   "cell_type": "code",
   "id": "71a28eeb-76b0-4a1e-919a-734563bf7b4f",
   "metadata": {
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": "session.write_pandas(df_live_customers, \"PS_DOCUWARE_LIVE_CUSTOMER_FEATURES\", auto_create_table=True, overwrite = True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9aa3a07c-4823-4c68-a1fc-ae38c355923f",
   "metadata": {
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": "import shap\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0f7add6e-6df4-4282-8300-c98d0919fd1d",
   "metadata": {
    "name": "cell65"
   },
   "source": "# Inferencing for Live customers to predict the churn duration"
  },
  {
   "cell_type": "code",
   "id": "3ff11f92-9fba-45e4-a1e3-937f736cf35c",
   "metadata": {
    "language": "python",
    "name": "cell19",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "y_pred_transformed = best_model.predict(input_features)\nremaining_months = y_pred_transformed/30\n#lifespan_months = np.exp(y_pred_transformed)\n\nfinal_predicted_df = pd.DataFrame({\n    \"CUST_ACCOUNT_NUMBER\": comprehensive_imputed_df_active[\"CUST_ACCOUNT_NUMBER\"].astype(str),\n    \"MONTHS_REMAINING\": remaining_months\n})\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "23c297fc-cf51-4cd8-8e78-0a88c073af30",
   "metadata": {
    "language": "python",
    "name": "cell94"
   },
   "outputs": [],
   "source": "final_predicted_df.head(50)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c7d86fce-883a-4a53-8568-4c5762ab657e",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": "raw_df_active_1 = raw_df_active[['CUST_ACCOUNT_NUMBER', 'FINAL_EARLIEST_DATE']].drop_duplicates()\n\nfinal_predicted_df = final_predicted_df.merge(raw_df_active_1, on='CUST_ACCOUNT_NUMBER', how='inner')\n\nfinal_predicted_df['CREATION_DATE'] = pd.Timestamp.today().date()\nfinal_predicted_df = final_predicted_df.rename({'FINAL_EARLIEST_DATE':'EARLIEST_START_DATE'})\n\n\nfinal_predicted_df['DAYS_ELAPSED'] = (pd.to_datetime(final_predicted_df['CREATION_DATE']) - pd.to_datetime(final_predicted_df['FINAL_EARLIEST_DATE'])).dt.days\nfinal_predicted_df['MONTHS_ELAPSED'] = (pd.to_datetime(final_predicted_df['CREATION_DATE']) - pd.to_datetime(final_predicted_df['FINAL_EARLIEST_DATE'])).dt.days/30\n#final_predicted_df['MONTHS_REMAINING'] = final_predicted_df['LIFESPAN_MONTHS'] #- final_predicted_df['MONTHS_ELAPSED']\nfinal_predicted_df['LIFESPAN_MONTHS'] = final_predicted_df['MONTHS_REMAINING'] + final_predicted_df['MONTHS_ELAPSED']\nfinal_predicted_df.head(100)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "44bb8a22-1572-474f-b742-89b73366aedd",
   "metadata": {
    "name": "cell66"
   },
   "source": "# Store Prediction for Live customers into DB"
  },
  {
   "cell_type": "code",
   "id": "8ce4dfde-e31d-4638-94b7-f96e31e466a9",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": "session.write_pandas(final_predicted_df, \"PS_DOCUWARE_PREDICTION_DATA_NEW\", auto_create_table=True, overwrite = True)\nsession.sql(\"INSERT INTO rus_aiml.PS_DOCUWARE_PREDICTION_DATA_ARCHIVE SELECT * FROM rus_aiml.PS_DOCUWARE_PREDICTION_DATA_NEW WHERE CREATION_DATE not in (select creation_date from rus_aiml.PS_DOCUWARE_PREDICTION_DATA_ARCHIVE)\")\n#session.sql(\"INSERT INTO PS_DOCUWARE_PREDICTION_DATA_ARCHIVE SELECT * FROM PS_DOCUWARE_PREDICTION_DATA_NEW WHERE CREATION_DATE != CURRENT_DATE()\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9cd536de-c570-49e6-b281-b1388c78952b",
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": "!pip install shap\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "681158ff-6753-4e2f-97c9-ab71c9a451b6",
   "metadata": {
    "language": "python",
    "name": "cell26"
   },
   "outputs": [],
   "source": "model_file = os.path.join('/tmp', 'xgb.joblib.gz')\nsession.file.get(\"@PS_DOCUWARE_CHURN/ps_docuware_churn_model/xgb.joblib.gz\", \"/tmp\")\nbest_model = load(model_file)\n#live_features = session.sql(\"SELECT * FROM RUS_AIML.PS_DOCUWARE_LIVE_CUSTOMER_FEATURES\").to_pandas()\nexplainer = shap.Explainer(best_model.predict, input_features)\nshap_values = explainer.shap_values(input_features)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18835767-cd4a-46e1-8f7d-936856dbe90e",
   "metadata": {
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": "shap_values.shape",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "08e37f81-de4c-43e7-b127-1d4bf779c13e",
   "metadata": {
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": "shap.summary_plot(shap_values)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1f8bcce4-5d4d-4dc9-9f09-6af001926575",
   "metadata": {
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": "shap.summary_plot(shap_values, plot_type='violin')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "edb2ddcc-316b-43c1-8716-259a45cb9593",
   "metadata": {
    "language": "python",
    "name": "cell33"
   },
   "outputs": [],
   "source": "feature_importance = global_shap_importance(best_models, input_features)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "248a416d-dad3-40dc-a648-5d5b13c7da32",
   "metadata": {
    "language": "python",
    "name": "cell34"
   },
   "outputs": [],
   "source": "feature_importance_top40 = feature_importance.head(40)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c58576f6-aaea-414b-811a-3be1a947820a",
   "metadata": {
    "name": "cell67"
   },
   "source": "# Plot of Shap to get top 40 important features for Live customer's churn prediction"
  },
  {
   "cell_type": "code",
   "id": "0103537c-99dc-4f4d-94d7-87ef04722b3c",
   "metadata": {
    "language": "python",
    "name": "cell35"
   },
   "outputs": [],
   "source": "# Sample data\ny = feature_importance_top40['importance']\nx = feature_importance_top40['features']\n\n# Create a figure and a set of subplots with a specified size\nfig, ax = plt.subplots(figsize=(40, 30))  # width=12 inches, height=8 inches\n\n# Plot the data on the axes\nax.plot(x, y)\n\n# Add labels and title\nax.set_xlabel('importance score')\nax.set_ylabel('Features')\nax.set_title('Graph of Top 40 Feature vs Importance While Predicting Input of 405 customers')\nplt.xticks(x, x, rotation=45, ha='right')\n#plt.savefig('C:/My Documents/work/Projects/Short Term Goal 2 - Customer Churn Prediction/ML Modeling Phase/Shap_Top_30_Importance_Features.png')\n# Show the plot\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fd985ed3-8ac6-4205-8ff9-d59ec8636a35",
   "metadata": {
    "language": "python",
    "name": "cell25"
   },
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [2, 4, 1, 3, 5]\n\nplt.plot(x, y, marker='o', linestyle='-', color='b', label='Data Points')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('My Graph')\nplt.legend()\n\ngraph_file = os.path.join(\"/tmp\", \"graph_new.jpg\")\nplt.savefig(graph_file)\nsession.file.put(graph_file, \"@PS_DOCUWARE_CHURN/graph_new.jpg\",overwrite=True)\n\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62572d86-c569-4a21-8e66-647200a6d1fd",
   "metadata": {
    "language": "sql",
    "name": "cell31"
   },
   "outputs": [],
   "source": " PUT /tmp/graph_new.jpg @PS_DOCUWARE_CHURN/ OVERWRITE=TRUE;",
   "execution_count": null
  }
 ]
}